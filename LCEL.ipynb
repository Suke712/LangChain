{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d54884",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59349f",
   "metadata": {},
   "source": [
    "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b4b5d",
   "metadata": {},
   "source": [
    "The `LLMChain` is the simplest chain originally introduced in LangChain (now deprecated). This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
    "\n",
    "* `prompt` ‚Äî a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
    "* `llm` ‚Äî the LLM we will be using to generate the output.\n",
    "* `output_parser` ‚Äî an optional output parser that will be used to parse the structured output of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a90db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env into environment\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_ENDPOINT'] = \"https://eu.api.smith.langchain.com \"\n",
    "os.environ['LANGSMITH_API_KEY'] =  os.getenv('LANGSMITH_API_KEY') or getpass('Enter your LangSmith API Key: ')\n",
    "os.environ['LANGSMITH_PROJECT'] = 'LCEL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7154f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or getpass(\n",
    "    \"Enter GOOGLE API Key: \"\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",      \n",
    "    temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "917e4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Give me a small report on {topic}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6f274",
   "metadata": {},
   "source": [
    "StrOutputParser: \n",
    "It parses the output of an LLM or ChatModel into a simple string (extracts and returns just the text).\n",
    "Useful for chains where you want the \"plain\" text content without metadata or objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ebb7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebe9a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1dfbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Small Report: Retrieval Augmented Generation (RAG) in LangChain\n",
      "\n",
      "### Introduction to RAG\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a powerful technique that enhances the capabilities of Large Language Models (LLMs) by allowing them to access and incorporate external, up-to-date, or proprietary information during the generation process. Traditional LLMs are limited to the data they were trained on, which can lead to hallucinations, outdated information, or an inability to answer questions about specific, private datasets. RAG addresses these limitations by first retrieving relevant information from a knowledge base and then using that information to ground the LLM's response.\n",
      "\n",
      "### LangChain's Role in RAG\n",
      "\n",
      "LangChain is a framework designed to simplify the development of applications powered by LLMs. It provides a modular and extensible set of tools and components that make building complex LLM workflows, including RAG pipelines, significantly easier. LangChain abstracts away much of the complexity involved in integrating various components like document loaders, text splitters, embedding models, vector stores, and LLMs.\n",
      "\n",
      "### Key Components and Workflow of RAG in LangChain\n",
      "\n",
      "A typical RAG pipeline in LangChain involves two main phases: **Indexing** (preparing the knowledge base) and **Retrieval & Generation** (answering user queries).\n",
      "\n",
      "#### Phase 1: Indexing (Building the Knowledge Base)\n",
      "\n",
      "1.  **Document Loading:** LangChain offers a wide array of `DocumentLoaders` to ingest data from various sources (PDFs, websites, databases, Notion, etc.). These loaders convert raw data into `Document` objects, which typically contain page content and metadata.\n",
      "2.  **Text Splitting:** LLMs have context window limitations, and embeddings work best on smaller, coherent chunks of text. `TextSplitters` (e.g., `RecursiveCharacterTextSplitter`) break down large documents into smaller, manageable chunks, ensuring semantic integrity.\n",
      "3.  **Embedding Generation:** Each text chunk is converted into a numerical vector (an embedding) using an `Embeddings` model (e.g., OpenAI Embeddings, HuggingFace Embeddings). These embeddings capture the semantic meaning of the text.\n",
      "4.  **Vector Store Storage:** The generated embeddings, along with their corresponding original text chunks, are stored in a `VectorStore` (e.g., Chroma, FAISS, Pinecone, Weaviate). This vector store acts as a searchable index for efficient retrieval.\n",
      "\n",
      "#### Phase 2: Retrieval & Generation (Answering Queries)\n",
      "\n",
      "1.  **User Query:** A user submits a question or prompt.\n",
      "2.  **Query Embedding:** The user's query is also converted into an embedding using the *same* embedding model used during indexing.\n",
      "3.  **Retrieval:** The query embedding is used to perform a similarity search in the `VectorStore`. The `VectorStore` returns the top-k most semantically similar text chunks (documents) from the knowledge base. LangChain's `Retriever` interface standardizes this process.\n",
      "4.  **Prompt Augmentation:** The retrieved relevant documents are then combined with the original user query to construct an augmented prompt. This prompt provides the LLM with the necessary context to formulate an accurate and grounded answer.\n",
      "5.  **LLM Generation:** The augmented prompt is sent to an `LLM` (e.g., `ChatOpenAI`, `HuggingFaceHub`). The LLM uses the provided context to generate a coherent and informed response.\n",
      "6.  **Orchestration (Chains/LCEL):** LangChain's `Chains` (like `RetrievalQA` or `create_retrieval_chain`) or the more flexible `LangChain Expression Language (LCEL)` are used to seamlessly connect all these steps. LCEL allows for building custom, highly optimized, and streaming-capable RAG pipelines with clear component separation.\n",
      "\n",
      "### Benefits of RAG with LangChain\n",
      "\n",
      "*   **Reduced Hallucinations:** LLMs are grounded in factual, retrieved information.\n",
      "*   **Access to Private/Up-to-Date Data:** Enables LLMs to answer questions beyond their training data.\n",
      "*   **Transparency:** Allows users to see the source documents used to generate the answer.\n",
      "*   **Modularity:** LangChain's component-based architecture makes it easy to swap out different loaders, splitters, embedding models, vector stores, and LLMs.\n",
      "*   **Rapid Prototyping:** Simplifies the development and iteration of complex RAG applications.\n",
      "*   **Scalability:** Supports various vector stores suitable for different scales and deployment needs.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "RAG is a transformative approach for building robust and reliable LLM applications, and LangChain stands out as a premier framework for implementing it. By providing a comprehensive toolkit for data ingestion, indexing, retrieval, and LLM integration, LangChain empowers developers to create intelligent systems that leverage external knowledge effectively, pushing the boundaries of what LLMs can achieve.\n"
     ]
    }
   ],
   "source": [
    "result = lcel_chain.invoke({\"topic\": \"RAG in LangChain\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6f46c",
   "metadata": {},
   "source": [
    "We can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b679dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Small Report: Retrieval Augmented Generation (RAG) in LangChain\n",
       "\n",
       "### Introduction to RAG\n",
       "\n",
       "Retrieval Augmented Generation (RAG) is a powerful technique that enhances the capabilities of Large Language Models (LLMs) by allowing them to access and incorporate external, up-to-date, or proprietary information during the generation process. Traditional LLMs are limited to the data they were trained on, which can lead to hallucinations, outdated information, or an inability to answer questions about specific, private datasets. RAG addresses these limitations by first retrieving relevant information from a knowledge base and then using that information to ground the LLM's response.\n",
       "\n",
       "### LangChain's Role in RAG\n",
       "\n",
       "LangChain is a framework designed to simplify the development of applications powered by LLMs. It provides a modular and extensible set of tools and components that make building complex LLM workflows, including RAG pipelines, significantly easier. LangChain abstracts away much of the complexity involved in integrating various components like document loaders, text splitters, embedding models, vector stores, and LLMs.\n",
       "\n",
       "### Key Components and Workflow of RAG in LangChain\n",
       "\n",
       "A typical RAG pipeline in LangChain involves two main phases: **Indexing** (preparing the knowledge base) and **Retrieval & Generation** (answering user queries).\n",
       "\n",
       "#### Phase 1: Indexing (Building the Knowledge Base)\n",
       "\n",
       "1.  **Document Loading:** LangChain offers a wide array of `DocumentLoaders` to ingest data from various sources (PDFs, websites, databases, Notion, etc.). These loaders convert raw data into `Document` objects, which typically contain page content and metadata.\n",
       "2.  **Text Splitting:** LLMs have context window limitations, and embeddings work best on smaller, coherent chunks of text. `TextSplitters` (e.g., `RecursiveCharacterTextSplitter`) break down large documents into smaller, manageable chunks, ensuring semantic integrity.\n",
       "3.  **Embedding Generation:** Each text chunk is converted into a numerical vector (an embedding) using an `Embeddings` model (e.g., OpenAI Embeddings, HuggingFace Embeddings). These embeddings capture the semantic meaning of the text.\n",
       "4.  **Vector Store Storage:** The generated embeddings, along with their corresponding original text chunks, are stored in a `VectorStore` (e.g., Chroma, FAISS, Pinecone, Weaviate). This vector store acts as a searchable index for efficient retrieval.\n",
       "\n",
       "#### Phase 2: Retrieval & Generation (Answering Queries)\n",
       "\n",
       "1.  **User Query:** A user submits a question or prompt.\n",
       "2.  **Query Embedding:** The user's query is also converted into an embedding using the *same* embedding model used during indexing.\n",
       "3.  **Retrieval:** The query embedding is used to perform a similarity search in the `VectorStore`. The `VectorStore` returns the top-k most semantically similar text chunks (documents) from the knowledge base. LangChain's `Retriever` interface standardizes this process.\n",
       "4.  **Prompt Augmentation:** The retrieved relevant documents are then combined with the original user query to construct an augmented prompt. This prompt provides the LLM with the necessary context to formulate an accurate and grounded answer.\n",
       "5.  **LLM Generation:** The augmented prompt is sent to an `LLM` (e.g., `ChatOpenAI`, `HuggingFaceHub`). The LLM uses the provided context to generate a coherent and informed response.\n",
       "6.  **Orchestration (Chains/LCEL):** LangChain's `Chains` (like `RetrievalQA` or `create_retrieval_chain`) or the more flexible `LangChain Expression Language (LCEL)` are used to seamlessly connect all these steps. LCEL allows for building custom, highly optimized, and streaming-capable RAG pipelines with clear component separation.\n",
       "\n",
       "### Benefits of RAG with LangChain\n",
       "\n",
       "*   **Reduced Hallucinations:** LLMs are grounded in factual, retrieved information.\n",
       "*   **Access to Private/Up-to-Date Data:** Enables LLMs to answer questions beyond their training data.\n",
       "*   **Transparency:** Allows users to see the source documents used to generate the answer.\n",
       "*   **Modularity:** LangChain's component-based architecture makes it easy to swap out different loaders, splitters, embedding models, vector stores, and LLMs.\n",
       "*   **Rapid Prototyping:** Simplifies the development and iteration of complex RAG applications.\n",
       "*   **Scalability:** Supports various vector stores suitable for different scales and deployment needs.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "RAG is a transformative approach for building robust and reliable LLM applications, and LangChain stands out as a premier framework for implementing it. By providing a comprehensive toolkit for data ingestion, indexing, retrieval, and LLM integration, LangChain empowers developers to create intelligent systems that leverage external knowledge effectively, pushing the boundaries of what LLMs can achieve."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41a3b9",
   "metadata": {},
   "source": [
    "### How Does the Pipe Operator Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52925091",
   "metadata": {},
   "source": [
    "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
    "\n",
    "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
    "\n",
    "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
    "\n",
    "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16b56c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kargs):\n",
    "            return other.invoke(self.func(*args, **kargs))\n",
    "        return Runnable(chained_func)\n",
    "    \n",
    "    def invoke(self, *args, **kargs):\n",
    "        return self.func(*args, **kargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa7d13",
   "metadata": {},
   "source": [
    "### üß© What This Class Does ‚Äî In Simple Terms\n",
    "\n",
    "`Runnable` lets you **chain functions together** using the `|` (pipe) operator ‚Äî just like you do in Unix commands or Pandas pipelines.\n",
    "\n",
    "So instead of:\n",
    "```python\n",
    "output = func3(func2(func1(data)))\n",
    "\n",
    "You can write:\n",
    "\n",
    "pipeline = Runnable(func1) | Runnable(func2) | Runnable(func3)\n",
    "result = pipeline.invoke(data)\n",
    "\n",
    "‚úÖ It‚Äôs cleaner\n",
    "‚úÖ It‚Äôs modular\n",
    "‚úÖ It builds a chain of steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187db550",
   "metadata": {},
   "source": [
    "### Breakdown of Each Part\n",
    "\n",
    "#### 1Ô∏è‚É£ The Constructor \n",
    "```python\n",
    "def __init__(self, func):\n",
    "    self.func = func\n",
    "```\n",
    "\n",
    "When you create a Runnable, you give it a function, and it stores that in self.func.\n",
    "\n",
    "#### 2Ô∏è‚É£ The invoke Method\n",
    "```python\n",
    "def invoke(self, *args, **kargs):\n",
    "    return self.func(*args, **kargs)\n",
    "```\n",
    "This just calls the stored function.\n",
    "\n",
    "invoke() is a wrapper that always executes the function the same way, which helps when chaining multiple Runnable objects.\n",
    "\n",
    "#### 3Ô∏è‚É£ The __or__ Method\n",
    "```python\n",
    "def __or__(self, other):\n",
    "    def chained_func(*args, **kargs):\n",
    "        return other.invoke(self.func(*args, **kargs))\n",
    "    return Runnable(chained_func)\n",
    "```\n",
    "This defines what happens when you use the | operator (pipe).\n",
    "\n",
    "When you write:\n",
    "\n",
    "A | B\n",
    "\n",
    "\n",
    "Python calls:\n",
    "```python\n",
    "A.__or__(B)\n",
    "```\n",
    "\n",
    "Here‚Äôs what it does:\n",
    "\n",
    "1. Defines a new function chained_func() that:\n",
    "    * Runs A‚Äôs function first (self.func)\n",
    "    * Passes its output to B‚Äôs invoke() method\n",
    "2.  Returns a new Runnable containing this chained function.\n",
    "\n",
    "This effectively builds a pipeline where:\n",
    "\n",
    "input ‚Üí A.func ‚Üí B.func ‚Üí output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193abd6e",
   "metadata": {},
   "source": [
    "\n",
    "### Why LangChain Uses This Pattern\n",
    "\n",
    "In LangChain, this Runnable pattern allows you to build data flow pipelines easily:\n",
    "```\n",
    "{\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    \"history\": lambda x: x[\"chat_history\"]\n",
    "} | prompt | llm\n",
    "```\n",
    "\n",
    "Each | connects one processing step to the next:\n",
    "\n",
    "Extract data ‚Üí format a prompt ‚Üí call the model ‚Üí return result\n",
    "\n",
    "### Summary\n",
    "```\n",
    "__init__: Stores the function\n",
    "invoke(): Calls the stored function\n",
    "__or__(): Chains this Runnable with another one so their functions run sequentially\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00291d4",
   "metadata": {},
   "source": [
    "With the `Runnable` class, we will be able to wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
    "\n",
    "First, let's create a few functions that we'll chain together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d44edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x+5\n",
    "\n",
    "def sub_five(x):\n",
    "    return x-5\n",
    "\n",
    "def mul_five(x):\n",
    "    return x*5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095c808",
   "metadata": {},
   "source": [
    "Now we wrap our functions with the `Runnable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e3f56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d61cad",
   "metadata": {},
   "source": [
    "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f45bdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = add_five_runnable.__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "chain.invoke(10)  # ((10 + 5) - 5) * 5 = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4268972",
   "metadata": {},
   "source": [
    "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3c9b015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
    "\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df1638",
   "metadata": {},
   "source": [
    "## LCEL `RunnableLambda`\n",
    "\n",
    "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d9cb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc5c84",
   "metadata": {},
   "source": [
    "We chain these together again with the pipe `|` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f8e192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6610630",
   "metadata": {},
   "source": [
    "And call them using the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9685e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e934b8",
   "metadata": {},
   "source": [
    "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5e25d",
   "metadata": {},
   "source": [
    "## LCEL `RunnableParallel` and `RunnablePassthrough`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272426c",
   "metadata": {},
   "source": [
    "LCEL provides us with various `Runnable` classes that allow us to control the flow of data and execution order through our chains. Two of these are `RunnableParallel` and `RunnablePassthrough`.\n",
    "\n",
    "* `RunnableParallel` ‚Äî allows us to run multiple `Runnable` instances in parallel. Acting almost as a Y-fork in the chain.\n",
    "\n",
    "* `RunnablePassthrough` ‚Äî allows us to pass through a variable to the next `Runnable` without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d505eb",
   "metadata": {},
   "source": [
    "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef302877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "#HuggingFaceEmbeddings creates vector embeddings for text using a free, local model.\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"half the info is here\",\n",
    "        \"DeepSeek-V3 was released in December 2024\",\n",
    "        \"dog is a domestic animal\"\n",
    "    ],\n",
    "    embedding=embedding_model\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
    "    ],\n",
    "    embedding=embedding_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907c248",
   "metadata": {},
   "source": [
    "DocArrayInMemorySearch builds two in-memory vector databases:\n",
    "* vecstore_a: \"half the info is here\", \"DeepSeek-V3 was released in December 2024\"\n",
    "* vecstore_b: \"the other half...\", \"the DeepSeek-V3 LLM is a mixture of experts...\"\n",
    "\n",
    "Each store can retrieve the most relevant chunk for a user question using semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5acf74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"\"\"Using the context provided, answer the user's question.\n",
    "        Context:\n",
    "        {context_a}\n",
    "        {context_b}\"\"\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1bd5da",
   "metadata": {},
   "source": [
    "Here we are wrapping our vector stores as retrievers so they can be fitted into one big retrieval variable to be used by the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cbee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever(search_kwargs={\"k\": 1}) # the top k documents with the highest similarity.\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\" : retriever_a, \"context_b\" : retriever_b, \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cb163",
   "metadata": {},
   "source": [
    "RunnableParallel: Runs multiple Runnables (functions, retrievers, etc.) in parallel.\n",
    "\n",
    "* context_a = retriever_a gets the most relevant doc(s) from vecstore_a\n",
    "\n",
    "* context_b = retriever_b gets from vecstore_b\n",
    "\n",
    "* question = RunnablePassthrough() simply returns the original input unchanged.\n",
    "\n",
    "Mapping:\n",
    "When you invoke this chain with a question (e.g., \"what architecture does the model DeepSeek released in december use?\"):\n",
    "\n",
    "This string is:\n",
    "\n",
    "* Passed to both retrievers as the query, getting matches for each.\n",
    "\n",
    "* Also passed as is to \"question\" via RunnablePassthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27efdad",
   "metadata": {},
   "source": [
    "The result of retrieval is a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c859497d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_a': [Document(metadata={}, page_content='DeepSeek-V3 was released in December 2024')],\n",
       " 'context_b': [Document(metadata={}, page_content='the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters'),\n",
       "  Document(metadata={}, page_content='the other half of the info is here')],\n",
       " 'question': 'what architecture does the model DeepSeek released in december use?'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval.invoke(\"what architecture does the model DeepSeek released in december use?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e7fad",
   "metadata": {},
   "source": [
    "This dictionary is used to fill the template variables in your prompt.\n",
    "\n",
    "llm generates an answer based on the expanded prompt.\n",
    "\n",
    "output_parser (e.g., StrOutputParser()) extracts the plain text from the LLM response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ae2d4",
   "metadata": {},
   "source": [
    "The chain will look something like this:\n",
    "\n",
    "![](https://github.com/aurelio-labs/langchain-course/blob/main/assets/lcel-flow.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57f98353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The DeepSeek model released in December (DeepSeek-V3) uses a mixture of experts architecture.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = retrieval | prompt | llm | output_parser\n",
    "\n",
    "chain.invoke(\"what architecture does the model DeepSeek released in december use?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613e9a4",
   "metadata": {},
   "source": [
    "With that we've seen how we can use `RunnableParallel` and `RunnablePassthrough` to control the flow of data and execution order through our chains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf5221",
   "metadata": {},
   "source": [
    "#### Vector Database\n",
    "\n",
    "A **vector database** (or vector store) is a database specialized for storing vector representations (**embeddings**) of documents, chunks, or other items.\n",
    "\n",
    "- **Semantic Search:** Lets you search for similar items based on vector similarity (not just keywords or exact match).\n",
    "- **Example:** Given a user query, you can find the most *semantically* similar passage, even if it uses different words or phrasing.\n",
    "- **Examples of vector database backends:** FAISS, Pinecone, Chroma, Weaviate, DocArrayInMemorySearch.\n",
    "\n",
    "**In your code:**\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts([...], embedding=embedding_model)\n",
    "\n",
    "Here, you are storing documents internally as embeddings, which enables semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "#### `.as_retriever()`\n",
    "\n",
    "The `.as_retriever()` method converts a vector store object into a **retriever interface**.\n",
    "\n",
    "- Allows you to use the vector store in a standard way for question answering, RAG, etc.\n",
    "- **Retriever objects** in LangChain are *callable*: you can `.invoke(question)` to return relevant chunks.\n",
    "- Includes configuration like `search_kwargs` for how many documents to fetch (`k`), filters, retrieval mode, etc.\n",
    "\n",
    "**In your code:**\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "output = retriever_a.invoke(\"What are DeepSeek-V3's details?\")\n",
    "\n",
    "This finds the *k* most similar documents from `vecstore_a` based on embedding similarity to your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68add5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='DeepSeek-V3 was released in December 2024')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_a.invoke(\"What are DeepSeek-V3's details?\") # k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ec4b447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters'),\n",
       " Document(metadata={}, page_content='the other half of the info is here')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_b.invoke(\"What are DeepSeek-V3's details?\") # k is not set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
