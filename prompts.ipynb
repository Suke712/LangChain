{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc13433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_ENDPOINT'] = \"https://eu.api.smith.langchain.com \"\n",
    "os.environ['LANGSMITH_API_KEY'] =  os.getenv('LANGSMITH_API_KEY') or getpass('Enter your LangSmith API Key: ')\n",
    "os.environ['LANGSMITH_PROJECT'] = 'LangChain-LangSmith-Demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15df00fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or getpass(\n",
    "    \"Enter GOOGLE API Key: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed9079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",      \n",
    "    temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d494357",
   "metadata": {},
   "source": [
    "The `temperature` parameter controls the randomness of the LLM's output. A temperature of `0.0` makes an LLM's output more determinstic which _in theory_ should lead to a lower likelihood of hallucination.\n",
    "\n",
    "Now, the question here may be, _why would we ever not use `temperature=0.0`?_ The answer to that is that sometimes a little bit of randomness can useful. Randomness tends to translate to text that feels more human and creative, so if we'd like an LLM to help us write an article or even a poem, that lack of determinism becomes a feature rather than a bug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99fa9b4",
   "metadata": {},
   "source": [
    "## Basic Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d639b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98c4a0",
   "metadata": {},
   "source": [
    "LangChain uses a `ChatPromptTemplate` object to format the various prompt types into a single list which will be passed to our LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de03704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6981cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e950e8c",
   "metadata": {},
   "source": [
    "When we call the template it will expect us to provide two variables, the `context` and the `query`. Both of these variables are pulled from the strings we wrote, as LangChain interprets curly-bracket syntax (ie `{context}` and `{query}`) as indicating a dynamic variable that we expect to be inserted at query time. We can see that these variables have been picked up by our template object by viewing it's `input_variables` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "333f565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44046a05",
   "metadata": {},
   "source": [
    "We can also view the structure of the messages (currently _prompt templates_) that the `ChatPromptTemplate` will construct by viewing the `messages` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723054a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6d0e1",
   "metadata": {},
   "source": [
    "From this, we can see that each tuple provided when using `ChatPromptTemplate.from_messages` becomes an individual prompt template itself. Within each of these tuples, the first value defines the _role_ of the message, which is typically `system`, `human`, or `ai`. Using these tuples is shorthand for the following, more explicit code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5059bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad100cbb",
   "metadata": {},
   "source": [
    "We can see the structure of this new chat prompt template is identical to our previous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57172512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82dc920c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428a91b",
   "metadata": {},
   "source": [
    "`prompt_template.format_messages()` produces a structured list of message objects (such as system and human), containing the formatted content and type. This allows us to preview exactly what the LLM will receive as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "132bdcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system : \n",
      "Answer the user's query based on the context below.\n",
      "If you cannot answer the question using the\n",
      "provided information answer with \"I don't know\".\n",
      "\n",
      "Context: This is a test context.\n",
      "\n",
      "human : This the user's query.\n"
     ]
    }
   ],
   "source": [
    "formatted = prompt_template.format_messages(query=\"This the user's query.\", context = \"This is a test context.\")\n",
    "\n",
    "for msg in formatted:\n",
    "    print(f\"{msg.type} : {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b647b4",
   "metadata": {},
   "source": [
    "We'll setup the pipeline to consume two variables when our LLM pipeline is called, `query` and `context`, we'll feed them into our chat prompt template, and then invoke our LLM with our formatted messages.\n",
    "\n",
    "Although that sounds complicated, all we're doing is connecting our `prompt_template` and `llm`. We do this with **L**ang**C**hain **E**xpression **L**anguage (LCEL), which uses the `|` operator to connect our each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d568eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"context\": lambda x: x[\"context\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "944619ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"LangChain is a framework for developing applications powered by language models. \n",
    "It can be used for chatbots, Generative Question-Answering (GQA), summarization, and much more.\"\"\"\n",
    "\n",
    "query = \"What is LangChain used for?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80228229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is used for developing applications powered by language models, including chatbots, Generative Question-Answering (GQA), and summarization.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--ca33ac44-657b-4956-a706-6f8148e0bfdb-0', usage_metadata={'input_tokens': 85, 'output_tokens': 90, 'total_tokens': 175, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 61}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"query\": query, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb682e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--e5e79dc5-502a-4a25-8445-1a5940896c49-0', usage_metadata={'input_tokens': 83, 'output_tokens': 105, 'total_tokens': 188, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 99}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"query\": \"What is LangSmith?\", \"context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4a941",
   "metadata": {},
   "source": [
    "## Few Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8c679",
   "metadata": {},
   "source": [
    "Many **S**tate-**o**f-**t**he-**A**rt (SotA) LLMs are incredible at instruction following. Meaning that it requires much less effort to get the intended output or behavior from these models than is the case for older LLMs and smaller LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4b42c",
   "metadata": {},
   "source": [
    "Before creating an example let's first see how to use LangChain's few shot prompting objects. We will provide multiple examples and we'll feed them in as sequential human and ai messages so we setup the template like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f7f3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a40c9a",
   "metadata": {},
   "source": [
    "Then we define a list of examples with dictionaries containing the correct `input` and `output` keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b241276",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7597e4",
   "metadata": {},
   "source": [
    "We then feed both of these into our `FewShotChatMessagePromptTemplate` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ab491e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bbf9c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Here is query #1\n",
      "AI: Here is the answer #1\n",
      "Human: Here is query #2\n",
      "AI: Here is the answer #2\n",
      "Human: Here is query #3\n",
      "AI: Here is the answer #3\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt.format()) # here is the formatted prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1bc1e4",
   "metadata": {},
   "source": [
    "Using this we can provide different sets of examples or even different individual example_prompt templates to the FewShotChatMessagePromptTemplate object to build our prompt structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814111fc",
   "metadata": {},
   "source": [
    "### A Few-Shot Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "947bc8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nAlways answer in markdown format. When doing so please\\nprovide headers, short summaries, follow with bullet\\npoints, then conclude.\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "prompt_template.messages[0].prompt.template = new_system_prompt\n",
    "prompt_template.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55ee603d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is LangChain used for?'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "40e469f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LangChain Applications\n",
      "\n",
      "LangChain is a versatile framework designed for building applications that leverage language models. It offers a wide range of uses, enabling developers to create sophisticated AI-powered tools.\n",
      "\n",
      "**Summary:**\n",
      "LangChain is primarily used for developing various applications powered by language models.\n",
      "\n",
      "**Key Uses:**\n",
      "*   **Chatbots:** Creating interactive conversational agents.\n",
      "*   **Generative Question-Answering (GQA):** Developing systems that can generate answers to questions based on provided information.\n",
      "*   **Summarization:** Building tools that can condense longer texts into shorter, coherent summaries.\n",
      "*   **Much more:** The framework's flexibility allows for a broad spectrum of other language model-powered applications.\n",
      "\n",
      "**Conclusion:**\n",
      "In essence, LangChain serves as a foundational tool for anyone looking to develop applications that harness the power of language models, from simple chatbots to complex generative AI systems.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'query': query, 'context': context}).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a6b45",
   "metadata": {},
   "source": [
    "We can display our markdown nicely with IPython like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "873de363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### LangChain Applications\n",
       "\n",
       "LangChain is a versatile framework designed for building applications that leverage language models. It offers a wide range of uses, enabling developers to create sophisticated AI-powered tools.\n",
       "\n",
       "**Summary:**\n",
       "LangChain is primarily used for developing various applications powered by language models.\n",
       "\n",
       "**Key Uses:**\n",
       "*   **Chatbots:** Creating interactive conversational agents.\n",
       "*   **Generative Question-Answering (GQA):** Developing systems that can generate answers to questions based on provided information.\n",
       "*   **Summarization:** Building tools that can condense longer texts into shorter, coherent summaries.\n",
       "*   **Much more:** The framework's flexibility allows for a broad spectrum of other language model-powered applications.\n",
       "\n",
       "**Conclusion:**\n",
       "In essence, LangChain serves as a foundational tool for anyone looking to develop applications that harness the power of language models, from simple chatbots to complex generative AI systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3cbb53",
   "metadata": {},
   "source": [
    "This is not bad, but also not quite the format we wanted. We could try improving our initial prompting instructions, but when this doesn't work we can move on to our few-shot prompting. We want to build something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc60a5",
   "metadata": {},
   "source": [
    "We have already defined our `example_prompt` so now we just change our `examples` to use some examples of a user asking a question and the LLM answering in the exact markdown format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "883389f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can you explain gravity?\",\n",
    "        \"output\": (\n",
    "            \"## Gravity\\n\\n\"\n",
    "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
    "            \"### Discovery\\n\\n\"\n",
    "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
    "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
    "            \"### In General Relativity\\n\\n\"\n",
    "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
    "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
    "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
    "            \"### Gravitons\\n\\n\"\n",
    "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
    "            \"* They have not yet been detected.\\n\\n\"\n",
    "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": (\n",
    "            \"## France\\n\\n\"\n",
    "            \"The capital of France is Paris.\\n\\n\"\n",
    "            \"### Origins\\n\\n\"\n",
    "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
    "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
    "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
    "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
    "        )\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6117add",
   "metadata": {},
   "source": [
    "We feed these into our `FewShotChatMessagePromptTemplate` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb969e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7f62f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Human: Can you explain gravity?\n",
       "AI: ## Gravity\n",
       "\n",
       "Gravity is one of the fundamental forces in the universe.\n",
       "\n",
       "### Discovery\n",
       "\n",
       "* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n",
       "* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n",
       "\n",
       "### In General Relativity\n",
       "\n",
       "* Gravity is described as the curvature of spacetime.\n",
       "* The more massive an object is, the more it curves spacetime.\n",
       "* This curvature is what causes objects to fall towards each other.\n",
       "\n",
       "### Gravitons\n",
       "\n",
       "* Gravitons are hypothetical particles that mediate the force of gravity.\n",
       "* They have not yet been detected.\n",
       "\n",
       "**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n",
       "\n",
       "\n",
       "Human: What is the capital of France?\n",
       "AI: ## France\n",
       "\n",
       "The capital of France is Paris.\n",
       "\n",
       "### Origins\n",
       "\n",
       "* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n",
       "* The Romans named the city Lutetia, which means \"the place where the river turns\".\n",
       "* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n",
       "\n",
       "**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(few_shot_prompt.format()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b95ade",
   "metadata": {},
   "source": [
    "We then pull all of this together with our system prompt and final user query to create our final prompt and feed it into our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24bffba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': 'Can you explain gravity?', 'output': '## Gravity\\n\\nGravity is one of the fundamental forces in the universe.\\n\\n### Discovery\\n\\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n### In General Relativity\\n\\n* Gravity is described as the curvature of spacetime.\\n* The more massive an object is, the more it curves spacetime.\\n* This curvature is what causes objects to fall towards each other.\\n\\n### Gravitons\\n\\n* Gravitons are hypothetical particles that mediate the force of gravity.\\n* They have not yet been detected.\\n\\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n'}, {'input': 'What is the capital of France?', 'output': '## France\\n\\nThe capital of France is Paris.\\n\\n### Origins\\n\\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world\\'s greatest cultural and economic centres.\\n\\n'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ae20fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", new_system_prompt),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f850728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nAlways answer in markdown format. When doing so please\\nprovide headers, short summaries, follow with bullet\\npoints, then conclude.\\n\\nContext: {context}\\n'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'Can you explain gravity?', 'output': '## Gravity\\n\\nGravity is one of the fundamental forces in the universe.\\n\\n### Discovery\\n\\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n### In General Relativity\\n\\n* Gravity is described as the curvature of spacetime.\\n* The more massive an object is, the more it curves spacetime.\\n* This curvature is what causes objects to fall towards each other.\\n\\n### Gravitons\\n\\n* Gravitons are hypothetical particles that mediate the force of gravity.\\n* They have not yet been detected.\\n\\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n'}, {'input': 'What is the capital of France?', 'output': '## France\\n\\nThe capital of France is Paris.\\n\\n### Origins\\n\\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world\\'s greatest cultural and economic centres.\\n\\n'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "16c10661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nAlways answer in markdown format. When doing so please\\nprovide headers, short summaries, follow with bullet\\npoints, then conclude.\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " FewShotChatMessagePromptTemplate(examples=[{'input': 'Can you explain gravity?', 'output': '## Gravity\\n\\nGravity is one of the fundamental forces in the universe.\\n\\n### Discovery\\n\\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n### In General Relativity\\n\\n* Gravity is described as the curvature of spacetime.\\n* The more massive an object is, the more it curves spacetime.\\n* This curvature is what causes objects to fall towards each other.\\n\\n### Gravitons\\n\\n* Gravitons are hypothetical particles that mediate the force of gravity.\\n* They have not yet been detected.\\n\\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n'}, {'input': 'What is the capital of France?', 'output': '## France\\n\\nThe capital of France is Paris.\\n\\n### Origins\\n\\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world\\'s greatest cultural and economic centres.\\n\\n'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512ccb5",
   "metadata": {},
   "source": [
    "Now feed this back into our pipeline(chain):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5979cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"context\": lambda x: x[\"context\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75ee7ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## LangChain Uses\n",
       "\n",
       "LangChain is a framework designed for building applications that leverage the power of language models.\n",
       "\n",
       "### Applications\n",
       "\n",
       "*   **Chatbots**: Developing interactive conversational agents.\n",
       "*   **Generative Question-Answering (GQA)**: Creating systems that can generate answers to questions based on provided information.\n",
       "*   **Summarization**: Tools for condensing longer texts into shorter, coherent summaries.\n",
       "*   **Much more**: Its capabilities extend beyond these specific examples to various other language model-powered applications.\n",
       "\n",
       "**To conclude**, LangChain provides a versatile toolkit for developers to create a wide range of applications utilizing language models, from conversational AI to information extraction and content generation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke({\"query\": query, \"context\": context})\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d79e54",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f5f76",
   "metadata": {},
   "source": [
    "We'll take a look at one more commonly used prompting technique called _chain of thought_ (CoT). CoT is a technique that encourages the LLM to think through the problem step by step before providing an answer. The idea being that by breaking down the problem into smaller steps, the LLM is more likely to arrive at the correct answer and we are less likely to see hallucinations.\n",
    "\n",
    "To implement CoT we don't need any specific LangChain objects, instead we are simply modifying how we instruct our LLM within the system prompt. We will ask the LLM to list the problems that need to be solved, to solve each problem individually, and then to arrive at the final answer.\n",
    "\n",
    "Let's first test our LLM _without_ CoT prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "24a79663",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "You MUST answer the question directly without using chain of thoughts and any other\n",
    "text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", no_cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e05dd",
   "metadata": {},
   "source": [
    "Nowadays most LLMs are trained to use CoT prompting by default, so we actually need to instruct it not to do so for this example which is why we added `\"You MUST answer the question directly without any other text or explanation.\"` to our system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "017a753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"How many keystrokes are needed to type the numbers from 1 to 1000?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "67e629a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2893\n"
     ]
    }
   ],
   "source": [
    "no_cot_chain = no_cot_prompt_template | llm\n",
    "no_cot_result = no_cot_chain.invoke({\"query\": query}).content\n",
    "print(no_cot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edd58a",
   "metadata": {},
   "source": [
    "The actual answer is `2893` which is correct, but sometimes the LLM _without_ CoT might hallucinats and give us a guess. Now, we can add explicit CoT prompting to our system prompt to see if we can get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "489fcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  subproblems that need to be solved to answer the\n",
    "  question.\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "cot_chain = cot_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "60be2270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine the total number of keystrokes needed to type the numbers from 1 to 1000, we need to break down the problem based on the number of digits in each number.\n",
       "\n",
       "### Subproblems to Solve:\n",
       "\n",
       "1.  **Calculate keystrokes for 1-digit numbers:** Numbers from 1 to 9.\n",
       "2.  **Calculate keystrokes for 2-digit numbers:** Numbers from 10 to 99.\n",
       "3.  **Calculate keystrokes for 3-digit numbers:** Numbers from 100 to 999.\n",
       "4.  **Calculate keystrokes for 4-digit numbers:** The number 1000.\n",
       "5.  **Sum all the calculated keystrokes** from the above subproblems.\n",
       "\n",
       "---\n",
       "\n",
       "### Solving Each Subproblem Individually:\n",
       "\n",
       "#### Subproblem 1: Keystrokes for 1-digit numbers (1 to 9)\n",
       "\n",
       "*   **Numbers:** 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
       "*   **Number of digits per number:** 1\n",
       "*   **Count of 1-digit numbers:** There are 9 numbers (from 1 to 9).\n",
       "*   **Total keystrokes for 1-digit numbers:** 9 numbers * 1 keystroke/number = **9 keystrokes**.\n",
       "\n",
       "#### Subproblem 2: Keystrokes for 2-digit numbers (10 to 99)\n",
       "\n",
       "*   **Numbers:** 10, 11, ..., 99\n",
       "*   **Number of digits per number:** 2\n",
       "*   **Count of 2-digit numbers:** (Last number - First number + 1) = (99 - 10 + 1) = 90 numbers.\n",
       "*   **Total keystrokes for 2-digit numbers:** 90 numbers * 2 keystrokes/number = **180 keystrokes**.\n",
       "\n",
       "#### Subproblem 3: Keystrokes for 3-digit numbers (100 to 999)\n",
       "\n",
       "*   **Numbers:** 100, 101, ..., 999\n",
       "*   **Number of digits per number:** 3\n",
       "*   **Count of 3-digit numbers:** (Last number - First number + 1) = (999 - 100 + 1) = 900 numbers.\n",
       "*   **Total keystrokes for 3-digit numbers:** 900 numbers * 3 keystrokes/number = **2700 keystrokes**.\n",
       "\n",
       "#### Subproblem 4: Keystrokes for 4-digit numbers (1000)\n",
       "\n",
       "*   **Numbers:** 1000\n",
       "*   **Number of digits per number:** 4\n",
       "*   **Count of 4-digit numbers:** There is only 1 number (1000).\n",
       "*   **Total keystrokes for 4-digit numbers:** 1 number * 4 keystrokes/number = **4 keystrokes**.\n",
       "\n",
       "---\n",
       "\n",
       "### Final Answer:\n",
       "\n",
       "Now, we sum the keystrokes from all the subproblems:\n",
       "\n",
       "*   Keystrokes for 1-digit numbers: 9\n",
       "*   Keystrokes for 2-digit numbers: 180\n",
       "*   Keystrokes for 3-digit numbers: 2700\n",
       "*   Keystrokes for 4-digit numbers: 4\n",
       "\n",
       "**Total Keystrokes = 9 + 180 + 2700 + 4 = 2893**\n",
       "\n",
       "Therefore, **2893** keystrokes are needed to type the numbers from 1 to 1000."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cot_result = cot_chain.invoke({\"query\": query}).content\n",
    "display(Markdown(cot_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9f6d6",
   "metadata": {},
   "source": [
    " Our LLM provides us with a final answer of `2893` which is correct. Finally, as mentioned most LLMs are now trained to use CoT prompting by default. So let's see what happens if we don't explicitly tell the LLM to use CoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ca3f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e9afb0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break this down by the number of digits in each number:\n",
       "\n",
       "1.  **1-digit numbers (1-9):**\n",
       "    *   There are 9 numbers (1, 2, 3, 4, 5, 6, 7, 8, 9).\n",
       "    *   Each requires 1 keystroke.\n",
       "    *   Total: 9 * 1 = **9 keystrokes**\n",
       "\n",
       "2.  **2-digit numbers (10-99):**\n",
       "    *   There are 90 numbers (99 - 10 + 1 = 90).\n",
       "    *   Each requires 2 keystrokes.\n",
       "    *   Total: 90 * 2 = **180 keystrokes**\n",
       "\n",
       "3.  **3-digit numbers (100-999):**\n",
       "    *   There are 900 numbers (999 - 100 + 1 = 900).\n",
       "    *   Each requires 3 keystrokes.\n",
       "    *   Total: 900 * 3 = **2700 keystrokes**\n",
       "\n",
       "4.  **4-digit numbers (1000):**\n",
       "    *   There is 1 number (1000).\n",
       "    *   It requires 4 keystrokes.\n",
       "    *   Total: 1 * 4 = **4 keystrokes**\n",
       "\n",
       "Now, add them all up:\n",
       "9 + 180 + 2700 + 4 = **2893 keystrokes**\n",
       "\n",
       "So, 2893 keystrokes are needed to type the numbers from 1 to 1000."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke({\"query\": query}).content\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ab8af",
   "metadata": {},
   "source": [
    "We get the _exact_ same result. The formatting isn't quite as nice but the CoT behavior is clearly there, and the LLM produces the correct final answer!\n",
    "\n",
    "CoT is useful not only for simple question-answering like this, but is also a fundamental component of many agentic systems which will often use CoT steps paired with tool use to solve very complex problems, this is what we see in Google's current flagship model `gemini-2.5-flash`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
