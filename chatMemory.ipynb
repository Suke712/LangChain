{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df911a02",
   "metadata": {},
   "source": [
    "# Conversational Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffa57b",
   "metadata": {},
   "source": [
    "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
    "\n",
    "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da1e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env into environment\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_ENDPOINT'] = \"https://eu.api.smith.langchain.com \"\n",
    "os.environ['LANGSMITH_API_KEY'] =  os.getenv('LANGSMITH_API_KEY') or getpass('Enter your LangSmith API Key: ')\n",
    "os.environ['LANGSMITH_PROJECT'] = 'LangChain-LangSmith-Demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9fe70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or getpass(\n",
    "    \"Enter GOOGLE API Key: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18aeab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",      \n",
    "    temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411441d4",
   "metadata": {},
   "source": [
    "## LangChain's Memory Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab07d3",
   "metadata": {},
   "source": [
    "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
    "\n",
    "Throughout the notebook we will be referring to these _older_ memory types and then rewriting them using the recommended `RunnableWithMessageHistory` class. We will learn about:\n",
    "\n",
    "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
    "* `ConversationBufferWindowMemory`: similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages.\n",
    "* `ConversationSummaryMemory`: rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
    "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types.\n",
    "\n",
    "We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e8706",
   "metadata": {},
   "source": [
    "## 1. `ConversationBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464aa2ec",
   "metadata": {},
   "source": [
    "`ConversationBufferMemory` is the simplest form of conversational memory, it is _literally_ just a place that we store messages, and then use to feed messages into our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e01e738",
   "metadata": {},
   "source": [
    "### `ConversationBufferMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100eee78",
   "metadata": {},
   "source": [
    "The `ConversationBufferMemory` type is deprecated. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
    "\n",
    "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df710bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt = \" You are a helpful assistant called Jarvis.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8415be4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['history', 'query'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000179EEAE7CE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=' You are a helpful assistant called Jarvis.'), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d05628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['history', 'query']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "986ce0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=' You are a helpful assistant called Jarvis.'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='history'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b6ff4",
   "metadata": {},
   "source": [
    "We can link our `prompt_template` and our `llm` together to create a pipeline via LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c945747",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30214f6d",
   "metadata": {},
   "source": [
    "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6210163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085841d5",
   "metadata": {},
   "source": [
    "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab522274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bda6a",
   "metadata": {},
   "source": [
    "Now we invoke our runnable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "260f4924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Sujal! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--a642589d-c04d-4774-982f-c038a0afbf36-0', usage_metadata={'input_tokens': 17, 'output_tokens': 44, 'total_tokens': 61, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Sujal.\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860a1fe",
   "metadata": {},
   "source": [
    "Our chat history will now be memorized and retrieved whenever we invoke our runnable with the same session ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc66f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Sujal.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--d8df0388-e86f-429b-8d6c-00ebff727c2f-0', usage_metadata={'input_tokens': 44, 'output_tokens': 33, 'total_tokens': 77, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 27}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f1daf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_123': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is Sujal.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Sujal! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--a642589d-c04d-4774-982f-c038a0afbf36-0', usage_metadata={'input_tokens': 17, 'output_tokens': 44, 'total_tokens': 61, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}}), HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Sujal.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--d8df0388-e86f-429b-8d6c-00ebff727c2f-0', usage_metadata={'input_tokens': 44, 'output_tokens': 33, 'total_tokens': 77, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 27}})])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41056e82",
   "metadata": {},
   "source": [
    "We have now recreated the `ConversationBufferMemory` type using the `RunnableWithMessageHistory`\n",
    "\n",
    "* RunnableWithMessageHistory: responsible for appending the chats \n",
    "* InMemoryChatMessageHistory: stores/appends when called, but does not manage the workflow itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39693d9",
   "metadata": {},
   "source": [
    "A normal call to llm just to check if it knows my name by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eecf5037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name. As an AI, I don't have access to personal information about you.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--6bc3025e-06f8-4f60-86a7-772d427167f6-0', usage_metadata={'input_tokens': 6, 'output_tokens': 503, 'total_tokens': 509, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 479}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1492bde",
   "metadata": {},
   "source": [
    "## 2. `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f5408",
   "metadata": {},
   "source": [
    "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
    "\n",
    "* More messages mean more tokens are sent with each request, more tokens increases latency _and_ cost.\n",
    "\n",
    "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or _\"forget\"_ information provided to them. Conciseness is key to high performing LLMs.\n",
    "\n",
    "* If we keep _all_ messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
    "\n",
    "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f37be9",
   "metadata": {},
   "source": [
    "### `ConversationBufferWindowMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e756d00e",
   "metadata": {},
   "source": [
    "To implement this memory type using the `RunnableWithMessageHistory` class, we can use the same approach as before. We define our `prompt_template` and `llm` as before, and then wrap our pipeline in a `RunnableWithMessageHistory` object.\n",
    "\n",
    "For the window feature, we need to define a custom version of the `InMemoryChatMessageHistory` class that removes any messages beyond the last `k` messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c95c9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"Chat message history that stores a window of messages.\"\"\"\n",
    "\n",
    "    messages: list[BaseModel] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to chat history.\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # Keep only the last k messages\n",
    "        self.messages = self.messages[-self.k :]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the chat history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "658133fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "faa43b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[              #We neet to tell the runnable about the new config parameters i.e.k and we just explicitly define session_id \n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"session_id\",\n",
    "            annotation = str,\n",
    "            name = \"Session ID\",\n",
    "            description = \"The session ID to use for retrieving chat history.\",\n",
    "            default = \"default_id\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"k\",\n",
    "            annotation = int,\n",
    "            name = \"Window Size\",\n",
    "            description = \"The number of messages to retain in the chat history window.\",\n",
    "            default = 4\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09b8b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=user_1 and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Sujal! It's nice to meet you.\\n\\nHow can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--a2628e98-e51c-4674-8668-dc8bc19c735c-0', usage_metadata={'input_tokens': 16, 'output_tokens': 37, 'total_tokens': 53, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 17}})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hello my name is Sujal.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_1\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f667d0",
   "metadata": {},
   "source": [
    "We can also modify the messages that are stored in memory by modifying the records inside the `chat_map` dictionary directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e39c547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map['user_1'].clear()\n",
    "\n",
    "# manually insert history\n",
    "chat_map['user_1'].add_user_message(\"Hello, my name is Sujal.\")\n",
    "chat_map['user_1'].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"user_1\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"user_1\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"user_1\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"user_1\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"user_1\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"user_1\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"user_1\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"user_1\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"user_1\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19838f96",
   "metadata": {},
   "source": [
    "Now let's see at which `k` value our LLM remembers our name — from the above we can already see that with `k=4` our name is not mentioned, so when running with `k=4` we should expect the LLM to forget our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b1f8243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=user_1 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name. As an AI, I don't have memory of past conversations or personal information about you.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--ab8cb8e3-05b8-4f69-bd25-dd4cbf96e8c2-0', usage_metadata={'input_tokens': 57, 'output_tokens': 60, 'total_tokens': 117, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 33}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Whart is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_1\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3b768",
   "metadata": {},
   "source": [
    "Now let's initialize a new session with `k=14`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16eb187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=user_2 and k=14\n",
      "Initializing BufferWindowMessageHistory with k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Sujal! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--205b94ab-fbf6-4fd7-8ff9-26d67aaca741-0', usage_metadata={'input_tokens': 17, 'output_tokens': 44, 'total_tokens': 61, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Sujal.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_2\", \"k\": 14}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e8dcd",
   "metadata": {},
   "source": [
    "We'll manually insert the remaining messages as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0db683c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Sujal.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi Sujal! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--205b94ab-fbf6-4fd7-8ff9-26d67aaca741-0', usage_metadata={'input_tokens': 17, 'output_tokens': 44, 'total_tokens': 61, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"user_2\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"user_2\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"user_2\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"user_2\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"user_2\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"user_2\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"user_2\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"user_2\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"user_2\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f6eb9",
   "metadata": {},
   "source": [
    "Now let's see if the LLM remembers our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a9d1b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=user_2 and k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Sujal.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--fe612b62-91af-4553-a25b-948504c02321-0', usage_metadata={'input_tokens': 136, 'output_tokens': 42, 'total_tokens': 178, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 36}})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_2\", \"k\": 14}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce22c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Sujal.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi Sujal! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--205b94ab-fbf6-4fd7-8ff9-26d67aaca741-0', usage_metadata={'input_tokens': 17, 'output_tokens': 44, 'total_tokens': 61, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Sujal.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--fe612b62-91af-4553-a25b-948504c02321-0', usage_metadata={'input_tokens': 136, 'output_tokens': 42, 'total_tokens': 178, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 36}})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map['user_2'].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11828855",
   "metadata": {},
   "source": [
    "That's it! We've rewritten our buffer window memory using the recommended `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4e5ac",
   "metadata": {},
   "source": [
    "## 3. `ConversationSummaryMemory`\n",
    "\n",
    "This memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c560a9d",
   "metadata": {},
   "source": [
    "### `ConversationSummaryMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed2980",
   "metadata": {},
   "source": [
    "Let's implement this memory type using the `RunnableWithMessageHistory` class. As with the window buffer memory, we need to define a custom implementation of the `InMemoryChatMessageHistory` class. We'll call this one `ConversationSummaryMessageHistory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fcc5a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"Chat message history that keeps a summary of the conversation.\"\"\"\n",
    "\n",
    "    messages: list[BaseModel] = Field(default_factory=list)\n",
    "    llm: ChatGoogleGenerativeAI = Field(default_factory=ChatGoogleGenerativeAI)\n",
    "\n",
    "    def __init__(self, llm: ChatGoogleGenerativeAI):\n",
    "        super().__init__(llm=llm)\n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Create a summary.\"\"\"\n",
    "\n",
    "        self.messages.extend(messages)\n",
    "        summary_promt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing Conversation Summary: {existing_summary}\" \\\n",
    "                \"\\n New Messages: {messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = llm.invoke(\n",
    "            summary_promt.format_messages(\n",
    "                #existing_summary=self.messages[0].content if self.messages else \"\",\n",
    "                existing_summary=self.messages,\n",
    "                messages= [msg.content for msg in messages]\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the chat history.\"\"\"\n",
    "        self.messages = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dcec6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "52aeebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[              \n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"session_id\",\n",
    "            annotation = str,\n",
    "            name = \"Session ID\",\n",
    "            description = \"The session ID to use for retrieving chat history.\",\n",
    "            default = \"default_id\"\n",
    "        ),\n",
    "\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"llm\",\n",
    "            annotation = ChatGoogleGenerativeAI,\n",
    "            name = \"LLM\",\n",
    "            description = \"The LLM to use for generating conversation summaries.\",\n",
    "            default = llm\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bc038",
   "metadata": {},
   "source": [
    "Now we invoke our runnable, this time passing a `llm` parameter via the `config` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "637bc721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Sujal, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--38a7e6f8-6b43-496f-88ed-fa7274af407c-0', usage_metadata={'input_tokens': 16, 'output_tokens': 36, 'total_tokens': 52, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 17}})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Sujal\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f267abb6",
   "metadata": {},
   "source": [
    "Let's see what summary was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b24d4d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='The conversation begins with Sujal introducing themselves. The AI responds by greeting Sujal and asking how it can help.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map['id_123'].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085dafb",
   "metadata": {},
   "source": [
    "Let's continue the conversation and see if the summary is updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "59561394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='That\\'s a fascinating area of study! Conversational memory is crucial for effective communication and understanding.\\n\\nWhen we talk about \"conversational memory,\" we can approach it from a few different angles, often overlapping. Are you interested primarily in:\\n\\n1.  **Human cognitive memory during conversations?** (e.g., how people remember what was said, who said it, the context, etc.)\\n2.  **Memory in AI/chatbot systems?** (e.g., how AI maintains context and recalls past turns)\\n3.  **Or perhaps something else?**\\n\\nAssuming you\\'re looking at the **human cognitive aspect**, here are some common ways researchers categorize or discuss types of conversational memory:\\n\\n*   **Content Memory:** Remembering *what* was said – the specific words, facts, or propositions exchanged.\\n*   **Source Memory:** Remembering *who* said what – attributing specific statements or ideas to the correct speaker. This is vital for understanding perspectives and credibility.\\n*   **Contextual Memory:** Remembering the circumstances surrounding the conversation – where it took place, when, the emotional tone, the purpose, or the overall topic.\\n*   **Episodic Memory:** Remembering the conversation as a specific event or experience, including the sequence of turns, the flow, and personal feelings associated with it.\\n*   **Semantic Memory:** Over time, the specific details of a conversation might fade, but the general gist, the main conclusions, or the new knowledge gained can be integrated into one\\'s semantic memory (general knowledge).\\n*   **Working Memory:** This isn\\'t a long-term type, but it\\'s critical *during* a conversation. It holds the most recent utterances and context active so you can process new information and formulate your response.\\n*   **Relational Memory:** Remembering the relationship dynamics or changes in relationships that occurred or were revealed during the conversation.\\n\\nThese types often interact. For example, remembering *who* said *what* (source + content) is a key part of episodic memory for a conversation.\\n\\nWhich of these areas are you most interested in exploring further, or do you have a specific theory or model of conversational memory in mind?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--b64708a4-69e2-4ac6-a133-ad4d66349ca4-0', usage_metadata={'input_tokens': 43, 'output_tokens': 965, 'total_tokens': 1008, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 512}})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d2e5f3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='The conversation begins with Sujal introducing themselves. The AI responds by greeting Sujal and asking how it can help. Sujal states they are researching different types of conversational memory. The AI acknowledges the topic as fascinating and crucial, then asks if Sujal is interested primarily in human cognitive memory during conversations, memory in AI/chatbot systems, or something else.\\n\\nAssuming the human cognitive aspect, the AI provides a detailed list of common types of conversational memory:\\n*   **Content Memory:** Remembering what was said.\\n*   **Source Memory:** Remembering who said what.\\n*   **Contextual Memory:** Remembering the circumstances of the conversation.\\n*   **Episodic Memory:** Remembering the conversation as a specific event.\\n*   **Semantic Memory:** Integrating the general gist or new knowledge gained.\\n*   **Working Memory:** Holding recent utterances and context during the conversation.\\n*   **Relational Memory:** Remembering relationship dynamics revealed during the conversation.\\n\\nThe AI notes that these types often interact (e.g., source + content is part of episodic memory) and asks Sujal which area they are most interested in exploring further, or if they have a specific theory or model in mind.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map['id_123'].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93753df",
   "metadata": {},
   "source": [
    "So far so good! Let's continue with a few more messages before returning to the name question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ece0a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63feb8df",
   "metadata": {},
   "source": [
    "Let's see the latest summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c6c283c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='The conversation began with Sujal introducing themselves and stating their research interest in different types of conversational memory. The AI responded by outlining various types of human cognitive conversational memory (Content, Source, Contextual, Episodic, Semantic, Working, and Relational Memory) and then asked Sujal to specify their interest.\\n\\nSujal clarified their interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`, which the AI identified as AI/chatbot system implementations, particularly common in frameworks like LangChain. The AI provided a detailed explanation of both:\\n\\n1.  **`ConversationBufferMemory`**: Stores the entire conversation history from the beginning, appending every new interaction. Its pros include complete context and simplicity, while cons include quickly exceeding LLM token limits, higher costs/latency, and potential for irrelevant information. It\\'s likened to a full transcript.\\n2.  **`ConversationBufferWindowMemory`**: Stores only the last `N` interactions (a \"window\"), dropping the oldest interaction when a new one occurs. Its pros include managing token limits, efficiency, and focusing on recent context. Cons include the loss of older context and the need for careful `k` value selection. It\\'s likened to remembering only the last few things said.\\n\\nThe AI then asked if Sujal was interested in implementation, practical implications, or other memory strategies. Sujal then reiterated that \"Buffer memory just stores the entire conversation,\" which the AI confirmed, emphasizing that `ConversationBufferMemory` keeps a complete, unedited transcript. Sujal then stated that \"Buffer window memory stores the last k messages, dropping the rest.\" The AI confirmed this understanding, reiterating that `ConversationBufferWindowMemory` stores only the last `k` messages, dropping the oldest when a new one arrives. The AI then summarized the two types: `ConversationBufferMemory` stores everything, while `ConversationBufferWindowMemory` stores only the last `k` messages. Finally, the AI asked Sujal if they wanted to discuss practical implications, look at implementation examples, or explore other AI conversational memory strategies.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887be61a",
   "metadata": {},
   "source": [
    "The information about our name has been maintained, so let's see if this is enough for our LLM to correctly recall our name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "126308ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Sujal.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--788e1a78-852d-48f0-8826-8047f3ae6d4c-0', usage_metadata={'input_tokens': 436, 'output_tokens': 41, 'total_tokens': 477, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 35}})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8e531",
   "metadata": {},
   "source": [
    "Perfect! We've successfully implemented the `ConversationSummaryMemory` type using the `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d54eb",
   "metadata": {},
   "source": [
    "## 4. `ConversationSummaryBufferMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922b08f",
   "metadata": {},
   "source": [
    "Our final memory type acts as a combination of `ConversationSummaryMemory` and `ConversationBufferMemory`. It keeps the buffer for the conversation up until the previous `n` tokens, anything beyond that limit is summarized then dropped from the buffer. Producing something like:\n",
    "\n",
    "\n",
    "```\n",
    "# ~~ a summary of previous interactions\n",
    "The user named Sujal introduced himself and the AI responded, introducing itself as an AI model called Jarvis.\n",
    "Sujal then said he was researching the different types of conversational memory and Jarvis asked for some\n",
    "examples.\n",
    "# ~~ the most recent messages\n",
    "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
    "AI: That's interesting, what's the difference?\n",
    "Human: Buffer memory just stores the entire conversation\n",
    "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
    "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
    "AI: Very cool!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c21bb",
   "metadata": {},
   "source": [
    "### `ConversationSummaryBufferMemory` with `RunnableWithMessageHistory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d1d3e",
   "metadata": {},
   "source": [
    "As with the previous memory types, we will implement this memory type again using the `RunnableWithMessageHistory` class. In our implementation we will modify the buffer window to be based on the number of messages rather than number of tokens. This tweak will make our implementation more closely aligned with original buffer window.\n",
    "\n",
    "We will implement all of this via a new `ConversationSummaryBufferMessageHistory` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "34200265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"Chat message history that keeps a summary and a buffer of recent messages.\"\"\"\n",
    "\n",
    "    messages: list[BaseModel] = Field(default_factory=list)\n",
    "    llm: ChatGoogleGenerativeAI = Field(default_factory=ChatGoogleGenerativeAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatGoogleGenerativeAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we\n",
    "        drop.\"\"\"\n",
    "\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "\n",
    "        #see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "\n",
    "        #add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        #check if we have more than k messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping oldest {len(self.messages) - self.k} messages\"\n",
    "            )\n",
    "\n",
    "            #pull out the old messages beyond the last k\n",
    "            old_messages = self.messages[:-self.k]\n",
    "            #and keep only the last k messages\n",
    "            self.messages = self.messages[-self.k :]\n",
    "\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to summarize, skipping summary step.\")\n",
    "            return\n",
    "        \n",
    "        #construct the summary chat messages\n",
    "        summary_promt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing Conversation Summary: {existing_summary}\\n New Messages: {old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = llm.invoke(\n",
    "            summary_promt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages= [msg.content for msg in old_messages]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the chat history.\"\"\"\n",
    "        self.messages = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38dfcd",
   "metadata": {},
   "source": [
    "Redefine the `get_chat_history` function to use our new `ConversationSummaryBufferMessageHistory` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c51c22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118a6bd",
   "metadata": {},
   "source": [
    "Setup our pipeline with new configurable fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3323d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"session_id\",\n",
    "            annotation = str,\n",
    "            name = \"Session ID\",\n",
    "            description = \"The session ID to use for retrieving chat history.\",\n",
    "            default = \"default_id\"\n",
    "        ),\n",
    "\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"llm\",\n",
    "            annotation = ChatGoogleGenerativeAI,\n",
    "            name = \"LLM\",\n",
    "            description = \"The LLM to use for generating conversation summaries.\",\n",
    "            default = llm\n",
    "        ),\n",
    "\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"k\",\n",
    "            annotation = int,\n",
    "            name = \"Window Size\",\n",
    "            description = \"The number of messages to retain in the chat history window.\",\n",
    "            default = 4\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "40effd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> No old messages to summarize, skipping summary step.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Sujal.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi Sujal! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--7999be07-08ca-4bd4-9938-8781b5289de5-0', usage_metadata={'input_tokens': 17, 'output_tokens': 44, 'total_tokens': 61, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}})]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Sujal.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    ")\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c27b8c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      ">> No old messages to summarize, skipping summary step.\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      ">> Found 6 messages, dropping oldest 2 messages\n",
      ">> New summary: Sujal introduced himself, and the other participant greeted him, expressed pleasure in meeting him, and asked how they could help.\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      ">> found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages\n",
      ">> New summary: Sujal introduced himself, and the other participant greeted him, expressed pleasure in meeting him, and asked how they could help. Sujal then stated he is researching the different types of conversational memory. The participant acknowledged this as a crucial area, explaining that conversational memory is a complex interplay of various memory systems, defined by the ability to process and retain information during a conversation, recall details from past conversations, and use that information to guide future interactions. They then detailed seven key types of memory that contribute to conversational memory: Working Memory, Episodic Memory, Semantic Memory, Autobiographical Memory, Prospective Memory, Emotional Memory, and Social/Relational Memory, emphasizing that these types interact rather than operating in isolation. The participant concluded by asking Sujal about his specific research angle (psychological, neurological, or AI/computational linguistics) to provide more targeted information.\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n",
      ">> found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages\n",
      ">> New summary: Sujal introduced himself, stating he is researching different types of conversational memory. The other participant greeted him, defined conversational memory as a complex interplay of various memory systems (processing/retaining information, recalling past details, guiding future interactions), and detailed seven key human memory types that contribute to it: Working, Episodic, Semantic, Autobiographical, Prospective, Emotional, and Social/Relational Memory.\n",
      "\n",
      "To provide more targeted information, the participant asked about Sujal's specific research angle. Sujal clarified that he is looking at \"ConversationBufferMemory and ConversationBufferWindowMemory,\" indicating his focus is on **Large Language Models (LLMs)** and AI frameworks.\n",
      "\n",
      "The participant then explained that in LLMs, \"memory\" refers to how the AI retains and utilizes past conversation turns, as LLMs are inherently stateless. They provided a detailed breakdown of the two memory types Sujal mentioned:\n",
      "\n",
      "1.  **`ConversationBufferMemory`**:\n",
      "    *   Stores *all* previous messages, appending new interactions to a growing buffer.\n",
      "    *   **Pros:** Provides full context, simple to implement.\n",
      "    *   **Cons:** Can exceed LLM context window limits, increases API costs, slows performance, and accumulates irrelevant information.\n",
      "\n",
      "2.  **`ConversationBufferWindowMemory`**:\n",
      "    *   Stores only the *last `k` interactions*, dropping the oldest as new ones are added to maintain a fixed size.\n",
      "    *   **Pros:** Manages context window effectively, is cost-effective, offers faster performance, and focuses on recent relevance.\n",
      "    *   **Cons:** Can lead to the loss of crucial older context and requires careful selection of the `k` value.\n",
      "\n",
      "The participant provided a comparison table highlighting differences in history stored, context, context window management, token usage/cost, performance, use cases, and risks for both. They also noted that these are fundamental building blocks, with more advanced memory types (like `ConversationSummaryMemory`) existing to combine benefits. The participant concluded by asking Sujal about his specific implementation goals or application interests.\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate([\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3603e790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Sujal introduced himself, stating he is researching different types of conversational memory. The other participant greeted him, defined conversational memory as a complex interplay of various memory systems (processing/retaining information, recalling past details, guiding future interactions), and detailed seven key human memory types that contribute to it: Working, Episodic, Semantic, Autobiographical, Prospective, Emotional, and Social/Relational Memory.\\n\\nTo provide more targeted information, the participant asked about Sujal\\'s specific research angle. Sujal clarified that he is looking at \"ConversationBufferMemory and ConversationBufferWindowMemory,\" indicating his focus is on **Large Language Models (LLMs)** and AI frameworks.\\n\\nThe participant then explained that in LLMs, \"memory\" refers to how the AI retains and utilizes past conversation turns, as LLMs are inherently stateless. They provided a detailed breakdown of the two memory types Sujal mentioned:\\n\\n1.  **`ConversationBufferMemory`**:\\n    *   Stores *all* previous messages, appending new interactions to a growing buffer.\\n    *   **Pros:** Provides full context, simple to implement.\\n    *   **Cons:** Can exceed LLM context window limits, increases API costs, slows performance, and accumulates irrelevant information.\\n\\n2.  **`ConversationBufferWindowMemory`**:\\n    *   Stores only the *last `k` interactions*, dropping the oldest as new ones are added to maintain a fixed size.\\n    *   **Pros:** Manages context window effectively, is cost-effective, offers faster performance, and focuses on recent relevance.\\n    *   **Cons:** Can lead to the loss of crucial older context and requires careful selection of the `k` value.\\n\\nThe participant provided a comparison table highlighting differences in history stored, context, context window management, token usage/cost, performance, use cases, and risks for both. They also noted that these are fundamental building blocks, with more advanced memory types (like `ConversationSummaryMemory`) existing to combine benefits. The participant concluded by asking Sujal about his specific implementation goals or application interests.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Yes, that's absolutely correct!\\n\\n**`ConversationBufferMemory`** does exactly that: it stores the **entire, complete conversation history** from start to finish. Every user input and every AI response is appended to a growing buffer.\\n\\nIt's like having a full, unedited transcript of everything that's been said.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--b4e7a6d8-b9bd-46c7-9a21-06f77daf8fca-0', usage_metadata={'input_tokens': 2071, 'output_tokens': 206, 'total_tokens': 2277, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 137}}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Precisely! You've got it exactly right.\\n\\n**`ConversationBufferWindowMemory`** stores only the **last `k` messages** (or interactions). As new messages come in, the oldest messages are dropped from the buffer to maintain that fixed window size.\\n\\nIt's the key difference that helps manage the length of the conversation history passed to the LLM.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--38333780-b997-4066-9df2-317069a103fd-0', usage_metadata={'input_tokens': 1456, 'output_tokens': 110, 'total_tokens': 1566, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 33}})]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map['id_123'].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed18887",
   "metadata": {},
   "source": [
    "There we go, we've successfully implemented the `ConversationSummaryBufferMemory` type using `RunnableWithMessageHistory`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
