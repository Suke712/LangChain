{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c95cf1",
   "metadata": {},
   "source": [
    "# Streaming With Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b29835",
   "metadata": {},
   "source": [
    "In this example, we will introduce LangChain's async streaming, allowing us to receive and view the tokens as they are generated by LLM. The use of streaming is typical in conversational interfaces and can provide a more natural experience for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cde0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_ENDPOINT'] = \"https://eu.api.smith.langchain.com \"\n",
    "os.environ['LANGSMITH_API_KEY'] =  os.getenv('LANGSMITH_API_KEY') or getpass('Enter your LangSmith API Key: ')\n",
    "os.environ['LANGSMITH_PROJECT'] = 'LangChain-Streaming'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda3db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sujal\\Projects\\LangChain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or getpass(\n",
    "    \"Enter GOOGLE API Key: \"\n",
    ")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",       # choose appropriate model name\n",
    "    temperature=0.0,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecbbfa",
   "metadata": {},
   "source": [
    "## Streaming with `astream`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e67afc",
   "metadata": {},
   "source": [
    "We will start by creating a aysnc stream from our LLM. We do this within an `async for` loop, allowing us to iterate through the chunks of data and use them as soon as the async `astream` method returns the tokens to us. By adding a pipe character `|` we can see the individual tokens that are generated. We set `flush` equal to `True` as this forces immediate output to the console, resulting in smoother streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8d2eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**NLP stands for Natural Language Processing.**\n",
      "\n",
      "It's a field of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language in a way that is valuable and meaningful. Think of it as the bridge between human| communication and computer understanding.\n",
      "\n",
      "### The Core Goal of NLP\n",
      "\n",
      "The primary goal of NLP is to make computers capable of processing and analyzing large amounts of natural language data. This includes both written text and spoken words. Ultimately, it aims to allow computers| to \"read\" and \"comprehend\" text and speech in a similar way humans do, and to respond intelligently.\n",
      "\n",
      "### How it Works (Simplified)\n",
      "\n",
      "NLP combines computational linguistics (rule-based modeling of human language) with machine learning and deep| learning models. It involves breaking down language into smaller, manageable pieces and then applying algorithms to extract meaning, identify patterns, and make predictions.\n",
      "\n",
      "### Key Tasks and Components of NLP\n",
      "\n",
      "NLP encompasses a wide range of tasks, including:\n",
      "\n",
      "*|   **Tokenization:** Breaking text into individual words or sentences.\n",
      "*   **Part-of-Speech Tagging (POS Tagging):** Identifying the grammatical role of each word (noun, verb, adjective, etc.).\n",
      "*   **Named| Entity Recognition (NER):** Identifying and classifying named entities in text (e.g., names of people, organizations, locations, dates).\n",
      "*   **Sentiment Analysis:** Determining the emotional tone or opinion expressed in a piece of text (positive,| negative, neutral).\n",
      "*   **Machine Translation:** Automatically translating text or speech from one language to another.\n",
      "*   **Text Summarization:** Condensing a longer text into a shorter, coherent summary.\n",
      "*   **Speech Recognition|:** Converting spoken language into written text (e.g., voice assistants).\n",
      "*   **Natural Language Generation (NLG):** Generating human-like text from structured data.\n",
      "*   **Question Answering:** Enabling systems to answer questions| posed in natural language.\n",
      "*   **Topic Modeling:** Discovering abstract \"topics\" that occur in a collection of documents.\n",
      "*   **Spam Detection:** Identifying unwanted emails or messages.\n",
      "\n",
      "### Real-World Applications of NLP\n",
      "\n",
      "You| encounter NLP every day, often without realizing it:\n",
      "\n",
      "*   **Virtual Assistants:** Siri, Alexa, Google Assistant use NLP for speech recognition and understanding your commands.\n",
      "*   **Machine Translation:** Google Translate, DeepL.\n",
      "*   **|Spam Filters:** Your email provider uses NLP to identify and filter out junk mail.\n",
      "*   **Search Engines:** Understanding your search queries and providing relevant results.\n",
      "*   **Chatbots and Customer Service:** Automated systems that can answer questions and assist| users.\n",
      "*   **Social Media Monitoring:** Analyzing public sentiment about brands or topics.\n",
      "*   **Grammar and Spell Checkers:** Tools like Grammarly.\n",
      "*   **Predictive Text:** On your smartphone keyboard.\n",
      "\n",
      "### Challenges| in NLP\n",
      "\n",
      "Despite its advancements, NLP faces significant challenges due to the complexity of human language:\n",
      "\n",
      "*   **Ambiguity:** Words and phrases can have multiple meanings depending on context (e.g., \"bank\" – river bank vs|. financial institution).\n",
      "*   **Context:** Understanding the surrounding information is crucial for accurate interpretation.\n",
      "*   **Sarcasm and Irony:** Difficult for computers to detect.\n",
      "*   **Slang and Idioms:** Const|antly evolving and often have non-literal meanings.\n",
      "*   **Grammar and Syntax Variations:** People express the same ideas in many different ways.\n",
      "*   **Data Scarcity:** For less common languages or highly specialized domains,| there might not be enough data to train robust models.\n",
      "\n",
      "In essence, NLP is about teaching computers to speak, listen, and understand like humans, opening up a vast array of possibilities for how we interact with technology and information.||"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "async for token in llm.astream(\"What is NLP?\"):\n",
    "    tokens.append(token)\n",
    "    print(token.content, end='|', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9bd71",
   "metadata": {},
   "source": [
    "Since we appended each token to the `tokens` list, we can also see what is inside each and every token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d19dd3e",
   "metadata": {},
   "source": [
    "Without streaming we have to wait for the entire output to complete before we see anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b9a29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Natural Language Processing (NLP)** is a subfield of artificial intelligence (AI) that focuses on enabling computers to **understand, interpret, and generate human language** in a way that is both meaningful and useful.\\n\\nIn simpler terms, NLP is about teaching computers to \"read,\" \"understand,\" and \"write\" human languages (like English, Spanish, Chinese, etc.) just like we do.\\n\\nHere\\'s a breakdown of what that entails:\\n\\n1.  **Understanding Human Language:** This is the core challenge. Human language is incredibly complex, full of nuances, ambiguities, context, slang, sarcasm, and grammatical rules that aren\\'t always explicit. NLP aims to break down this complexity so computers can:\\n    *   **Identify words and their meanings:** Even words with multiple meanings (e.g., \"bank\" – river bank vs. financial institution).\\n    *   **Understand sentence structure (syntax):** How words are arranged to form grammatically correct sentences.\\n    *   **Grasp the overall meaning (semantics):** What the sentence or text actually means.\\n    *   **Recognize context:** How the surrounding words or situation influence meaning.\\n    *   **Extract information:** Pull out key facts, entities (people, places, organizations), or relationships.\\n\\n2.  **Interpreting and Analyzing Language:** Once understood, NLP systems can then analyze the language for various purposes:\\n    *   **Sentiment Analysis:** Determining the emotional tone (positive, negative, neutral) of a piece of text.\\n    *   **Topic Modeling:** Identifying the main themes or topics within a large collection of documents.\\n    *   **Named Entity Recognition (NER):** Locating and classifying named entities in text into predefined categories (e.g., names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages).\\n\\n3.  **Generating Human Language:** This involves creating new, coherent, and grammatically correct text that sounds natural to humans.\\n    *   **Machine Translation:** Translating text from one language to another (e.g., Google Translate).\\n    *   **Text Summarization:** Condensing a longer text into a shorter, coherent summary.\\n    *   **Chatbots and Virtual Assistants:** Generating responses to user queries.\\n    *   **Content Generation:** Creating articles, reports, or creative writing.\\n\\n---\\n\\n**Why is NLP important?**\\n\\n*   **Bridging the Human-Computer Gap:** It allows us to interact with computers using our natural way of communicating, rather than having to learn specific programming languages or commands.\\n*   **Automating Tasks:** It enables automation of tasks like customer support, data entry, and information retrieval.\\n*   **Extracting Insights:** It helps businesses and researchers make sense of vast amounts of unstructured text data (e.g., customer reviews, social media posts, medical records).\\n*   **Improving Accessibility:** It powers tools like screen readers and voice assistants, making technology more accessible.\\n\\n---\\n\\n**Common Real-World Applications of NLP:**\\n\\n*   **Voice Assistants:** Siri, Alexa, Google Assistant (understanding your commands).\\n*   **Spam Filters:** Identifying and filtering unwanted emails.\\n*   **Search Engines:** Understanding your queries and finding relevant results.\\n*   **Machine Translation:** Google Translate, DeepL.\\n*   **Chatbots and Customer Service:** Automated responses to common questions.\\n*   **Grammar and Spell Checkers:** Tools like Grammarly.\\n*   **Sentiment Analysis:** Analyzing customer reviews or social media for brand perception.\\n*   **Text Summarization:** Condensing news articles or reports.\\n*   **Predictive Text/Autocorrect:** On your smartphone keyboard.\\n\\nIn essence, NLP is what makes computers \"smart\" enough to deal with the messy, beautiful, and complex world of human language.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'model_provider': 'google_genai'}, id='lc_run--fd91af49-b512-45aa-9314-3c99d25ce95c', usage_metadata={'input_tokens': 5, 'output_tokens': 1857, 'total_tokens': 1862, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1061}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is NLP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c608b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"**NLP stands for Natural Language Processing.**\\n\\nIt's a field of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language in a way that is valuable and meaningful. Think of it as the bridge between human\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--302253a8-79eb-4c1f-8181-5b05165a2170', usage_metadata={'input_tokens': 5, 'output_tokens': 1355, 'total_tokens': 1360, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1304}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e73c91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' communication and computer understanding.\\n\\n### The Core Goal of NLP\\n\\nThe primary goal of NLP is to make computers capable of processing and analyzing large amounts of natural language data. This includes both written text and spoken words. Ultimately, it aims to allow computers', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--302253a8-79eb-4c1f-8181-5b05165a2170', usage_metadata={'output_tokens': 50, 'input_tokens': 0, 'total_tokens': 50, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c005dd58",
   "metadata": {},
   "source": [
    "We can also merge multiple `AIMessageChunk` objects together with the `+` operator, creating a larger set of tokens / chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "effb9e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"**NLP stands for Natural Language Processing.**\\n\\nIt's a field of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language in a way that is valuable and meaningful. Think of it as the bridge between human communication and computer understanding.\\n\\n### The Core Goal of NLP\\n\\nThe primary goal of NLP is to make computers capable of processing and analyzing large amounts of natural language data. This includes both written text and spoken words. Ultimately, it aims to allow computers\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--302253a8-79eb-4c1f-8181-5b05165a2170', usage_metadata={'input_tokens': 5, 'output_tokens': 1405, 'total_tokens': 1410, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1304}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0] + tokens[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd182b9",
   "metadata": {},
   "source": [
    "A word of caution, there is nothing preventing you from merging tokens in the incorrect order, so be cautious to not output any token omelettes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d842e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='   **Tokenization:** Breaking text into individual words or sentences.\\n*   **Part-of-Speech Tagging (POS Tagging):** Identifying the grammatical role of each word (noun, verb, adjective, etc.).\\n*   **Named learning models. It involves breaking down language into smaller, manageable pieces and then applying algorithms to extract meaning, identify patterns, and make predictions.\\n\\n### Key Tasks and Components of NLP\\n\\nNLP encompasses a wide range of tasks, including:\\n\\n* to \"read\" and \"comprehend\" text and speech in a similar way humans do, and to respond intelligently.\\n\\n### How it Works (Simplified)\\n\\nNLP combines computational linguistics (rule-based modeling of human language) with machine learning and deep communication and computer understanding.\\n\\n### The Core Goal of NLP\\n\\nThe primary goal of NLP is to make computers capable of processing and analyzing large amounts of natural language data. This includes both written text and spoken words. Ultimately, it aims to allow computers**NLP stands for Natural Language Processing.**\\n\\nIt\\'s a field of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language in a way that is valuable and meaningful. Think of it as the bridge between human', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--302253a8-79eb-4c1f-8181-5b05165a2170', usage_metadata={'input_tokens': 5, 'output_tokens': 1555, 'total_tokens': 1560, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1304}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[4] + tokens[3] + tokens[2] + tokens[1] + tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9946c9",
   "metadata": {},
   "source": [
    "## Streaming with Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfd2d2",
   "metadata": {},
   "source": [
    "Streaming with agents, particularly the custom agent executor, is a little more complex. Let's begin by constructor a simple agent executor matching what we built in the agentsIntro.\n",
    "\n",
    "To construct the agent executor we need:\n",
    "\n",
    "* Tools\n",
    "* `ChatPromptTemplate`\n",
    "* Our LLM (already defined with `llm`)\n",
    "* An agent\n",
    "* Finally, the agent executor\n",
    "\n",
    "Let's start defining each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd635f",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b58d6",
   "metadata": {},
   "source": [
    "Now we will define a few tools to be used by an async agent executor. Our goal for tool-use in regards to streaming are:\n",
    "\n",
    "* The tool-use steps will be streamed in one big chunk, ie we do not return the tool use information token-by-token but instead it streams message-by-message.\n",
    "\n",
    "* The final LLM output _will_ be streamed token-by-token as we saw above.\n",
    "\n",
    "For these we need to define a few math tools and our final answer tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922b1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x\n",
    "\n",
    "@tool\n",
    "def final_answer(answer: str, tools_used: list[str]) -> dict:\n",
    "    \"\"\"Use this tool to provide a final answer to the user.\n",
    "    The answer should be in natural language as this will be provided\n",
    "    to the user directly. The tools_used must include a list of tool\n",
    "    names that were used within the `scratchpad`. You MUST use this tool\n",
    "    to conclude the interaction.\n",
    "    \"\"\"\n",
    "    return {\"answer\": answer, \"tools_used\": tools_used}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44a543",
   "metadata": {},
   "source": [
    "We'll need all of our tools in a list when defining our `agent` and `agent_executor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "934fc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, multiply, exponentiate, subtract, final_answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe0c52",
   "metadata": {},
   "source": [
    "### `ChatPromptTemplate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746107c",
   "metadata": {},
   "source": [
    "We will create our `ChatPromptTemplate`, using a system message, chat history, user input, and a scratchpad for intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfec12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You're a helpful assistant. When answering a user's question \"\n",
    "        \"you should first use one of the tools provided. After using a \"\n",
    "        \"tool the tool output will be provided back to you. You MUST \"\n",
    "        \"then use the final_answer tool to provide a final answer to the user. \"\n",
    "        \"DO NOT use the same tool more than once.\"\n",
    "    )),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e09464",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700036b",
   "metadata": {},
   "source": [
    "As before, we will define our `agent` with LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f103e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate, final_answer]\n",
    "\n",
    "# define the agent runnable\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf609c0",
   "metadata": {},
   "source": [
    "### Agent Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b9ed7",
   "metadata": {},
   "source": [
    "Finally, we will create the agent executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8d6add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# create tool name to function mapping\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.chat_history = []\n",
    "        self.agent:  RunnableSerializable = agent\n",
    "\n",
    "    def invoke(self, input: str) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            tool_call = self.agent.invoke({\n",
    "                    \"input\": input,\n",
    "                    \"chat_history\": self.chat_history,\n",
    "                    \"agent_scratchpad\": agent_scratchpad\n",
    "                })\n",
    "\n",
    "            agent_scratchpad.append(tool_call) # add tool call to scratchpad\n",
    "\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "            tool_call_id = tool_call.tool_calls[0][\"id\"]\n",
    "\n",
    "            tool_execution_output = name2tool[tool_name](**tool_args)\n",
    "        \n",
    "            #add the tool output to the agent scratchpad\n",
    "            tool_message = ToolMessage(\n",
    "                content = tool_execution_output,\n",
    "                tool_call_id = tool_call_id\n",
    "            )\n",
    "            agent_scratchpad.append(tool_message)\n",
    "            # add a print so we can see intermediate steps\n",
    "            print(f\"{count}: {tool_name}({tool_args}) => {tool_execution_output}\")\n",
    "            count += 1\n",
    "\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if tool_call.tool_calls[0][\"name\"] == \"final_answer\":\n",
    "                break\n",
    "\n",
    "        # add the final output to the chat history\n",
    "        final_output = tool_execution_output['answer']\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=final_output)\n",
    "        ])\n",
    "\n",
    "         # return the final answer in dict form\n",
    "        return json.dumps(tool_execution_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942aee4",
   "metadata": {},
   "source": [
    "Our `agent_executor` is now ready to use, let's quickly test it before adding streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80ebe983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: add({'y': 10, 'x': 10}) => 20\n",
      "1: final_answer({'tools_used': ['add'], 'answer': '10 + 10 = 20'}) => {'answer': '10 + 10 = 20', 'tools_used': ['add']}\n"
     ]
    }
   ],
   "source": [
    "agent_executor = CustomAgentExecutor()\n",
    "response = agent_executor.invoke(\n",
    "    \"What is 10 + 10?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc15bba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"answer\": \"10 + 10 = 20\", \"tools_used\": [\"add\"]}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7f86f",
   "metadata": {},
   "source": [
    "Let's modify our `agent_executor` to use streaming and parse the streamed output into a format that we can more easily work with.\n",
    "\n",
    "First, when streaming with our custom agent executor we will need to pass our callback handler to the agent on every new invocation. To make this simpler we can make the `callbacks` field a configurable field and this will allow us to initialize the agent using the `with_config` method, allowing us to pass the callback handler to the agent with every invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f0aec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  \n",
    "    temperature=0.0,\n",
    "    streaming=True\n",
    ").configurable_fields(\n",
    "    callbacks = ConfigurableField(\n",
    "    id=\"callbacks\",\n",
    "    name=\"callbacks\",\n",
    "    description=\"A list of callbacks to use for streaming\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d94bc",
   "metadata": {},
   "source": [
    "Callback:\n",
    "* A callback is a function (or object with a function) you pass to another code block so it can \"call back\" later when something happens. It’s a way to customize what happens when an event occurs, usually as a parameter.\n",
    "\n",
    "Callback handler:\n",
    "* In practice, a callback handler is any system/function/object that listens for and processes these callbacks—handling the actual work when triggered (for example: printing, logging, updating a UI, or streaming output).\n",
    "\n",
    "Everyday analogy:\n",
    "* You give someone your phone number (callback). When dinner is ready (event), they call you (handle the callback) so you can come eat (react to the event).\n",
    "\n",
    "The code is making the callbacks field in Gemini LLM configurable, so you (or others) can flexibly insert custom actions for model events (like streaming output or tracing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a5b4a",
   "metadata": {},
   "source": [
    "We reinitialize our `agent`, nothing changes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b7eb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the agent runnable\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68ecc7",
   "metadata": {},
   "source": [
    "Now, we will define our _custom_ callback handler. This will be a queue callback handler that will allow us to stream the output of the agent through an `asyncio.Queue` object and yield the tokens as they are generated elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406fb635",
   "metadata": {},
   "source": [
    "#### asyncio\n",
    "asyncio is Python’s built-in framework for writing concurrent programs using the async/await syntax, optimized for I/O-bound tasks like network calls, file/socket I/O, and subprocess management within a single thread via an event loop.​\n",
    "\n",
    "* Coroutines: Define asynchronous functions with `async def` and suspend/resume them with `await`, allowing other coroutines to run while one is waiting on I/O.\n",
    "* Task creation: `asyncio.create_task(coro)` schedules a coroutine to run concurrently; use await on the task or asyncio.gather to await multiple tasks.\n",
    "* Non-blocking waits: Use await `asyncio.sleep(n)` as a placeholder for I/O waits; never use time.sleep in async code as it blocks the event loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd05dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_core.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "\n",
    "class QueueCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler that puts tokens into a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, queue: asyncio.Queue):\n",
    "        self.queue = queue\n",
    "        self.final_answer_seen = False\n",
    "\n",
    "    async def __aiter__(self):\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                await asyncio.sleep(0.1)\n",
    "                continue\n",
    "            token_or_done = await self.queue.get()\n",
    "\n",
    "            if token_or_done == \"<<DONE>>\":\n",
    "                # this means we're done\n",
    "                return\n",
    "            if token_or_done == \"<<STEP_END>>\":\n",
    "                yield \"<<STEP_END>>\"\n",
    "                continue\n",
    "            if token_or_done:\n",
    "                yield token_or_done\n",
    "\n",
    "\n",
    "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put new token in the queue.\"\"\"\n",
    "        #print(f\"on_llm_new_token: {args}, {kwargs}\")\n",
    "        chunk = kwargs.get(\"chunk\")\n",
    "        if chunk:\n",
    "            # check for final_answer tool call\n",
    "            if tool_calls := chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
    "                if tool_calls[0][\"function\"][\"name\"] == \"final_answer\":\n",
    "                    # this will allow the stream to end on the next `on_llm_end` call\n",
    "                    self.final_answer_seen = True\n",
    "        self.queue.put_nowait(kwargs.get(\"chunk\"))\n",
    "        return\n",
    "\n",
    "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put Done in the queue to signal completion.\"\"\"\n",
    "        #print(f\"on_llm_end: {args}, {kwargs}\")\n",
    "        # this should only be used at the end of our agent execution, however LangChain\n",
    "        # will call this at the end of every tool call, not just the final tool call\n",
    "        # so we must only send the \"done\" signal if we have already seen the final_answer\n",
    "        # tool call\n",
    "        if self.final_answer_seen:\n",
    "            self.queue.put_nowait(\"<<DONE>>\")\n",
    "        else:\n",
    "            self.queue.put_nowait(\"<<STEP_END>>\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d526a5c",
   "metadata": {},
   "source": [
    "We can see how this works together in our `agent` invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "041fa9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"y\": 10, \"x\": 10}'}} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'} id='lc_run--b8720420-4f39-4c94-9170-93f7c23025cf' tool_calls=[{'name': 'add', 'args': {'y': 10, 'x': 10}, 'id': '3e8510e2-1a95-4d3a-9a36-e1e69d3499ca', 'type': 'tool_call'}] usage_metadata={'input_tokens': 392, 'output_tokens': 77, 'total_tokens': 469, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}} tool_call_chunks=[{'name': 'add', 'args': '{\"y\": 10, \"x\": 10}', 'id': '3e8510e2-1a95-4d3a-9a36-e1e69d3499ca', 'index': None, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={} id='lc_run--b8720420-4f39-4c94-9170-93f7c23025cf' chunk_position='last'\n"
     ]
    }
   ],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "async def stream(query: str):\n",
    "    response = agent.with_config(\n",
    "        callbacks=[streamer]\n",
    "    )\n",
    "    async for token in response.astream({\n",
    "        \"input\": query,\n",
    "        \"chat_history\": [],\n",
    "        \"agent_scratchpad\": []\n",
    "    }):\n",
    "        tokens.append(token)\n",
    "        print(token, flush=True)\n",
    "\n",
    "await stream(\"What is 10 + 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52372278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'y': 10, 'x': 10},\n",
       "  'id': '3e8510e2-1a95-4d3a-9a36-e1e69d3499ca',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0].tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1b8fd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"y\": 10, \"x\": 10}'}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--b8720420-4f39-4c94-9170-93f7c23025cf', tool_calls=[{'name': 'add', 'args': {'y': 10, 'x': 10}, 'id': '3e8510e2-1a95-4d3a-9a36-e1e69d3499ca', 'type': 'tool_call'}], usage_metadata={'input_tokens': 392, 'output_tokens': 77, 'total_tokens': 469, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 57}}, tool_call_chunks=[{'name': 'add', 'args': '{\"y\": 10, \"x\": 10}', 'id': '3e8510e2-1a95-4d3a-9a36-e1e69d3499ca', 'index': None, 'type': 'tool_call_chunk'}], chunk_position='last')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = tokens[0]\n",
    "\n",
    "for token in tokens[1:]:\n",
    "    tk += token\n",
    "\n",
    "tk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba0ae2f",
   "metadata": {},
   "source": [
    "Now we're seeing that the output is being streamed token-by-token. Because we're being streamed a tool call the `content` field is empty. Instead, we can see the tokens being added inside the `tool_calls` fields, within `id`, `function.name`, and `function.arguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f52509ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.runnables import RunnableSerializable\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")\n",
    "        )\n",
    "\n",
    "    async def invoke(self, input: str, streamer: QueueCallbackHandler, verbose: bool = False): # Changed to async generator\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        final_answer = None\n",
    "        tool_args_final = {}\n",
    "\n",
    "        while count < self.max_iterations:\n",
    "            # Create a new queue and streamer for each agent step\n",
    "            step_queue = asyncio.Queue()\n",
    "            step_streamer = QueueCallbackHandler(step_queue)\n",
    "\n",
    "            response = self.agent.with_config(callbacks=[step_streamer])\n",
    "\n",
    "            current_tool_call = None\n",
    "            # Consume tokens directly from the astream generator\n",
    "            async for token in response.astream({\n",
    "                \"input\": input,\n",
    "                \"chat_history\": self.chat_history,\n",
    "                \"agent_scratchpad\": agent_scratchpad\n",
    "            }):\n",
    "                if token == \"<<DONE>>\":\n",
    "                    # This should not happen here based on QueueCallbackHandler logic,\n",
    "                    # but handle defensively\n",
    "                    break\n",
    "                # The step_streamer handles putting tokens into the step_queue\n",
    "                # We now consume from the step_streamer's aiter which yields the tokens\n",
    "                # and \"<<STEP_END>>\" or \"<<DONE>>\" signals\n",
    "\n",
    "                if token == \"<<STEP_END>>\":\n",
    "                    yield \"<<STEP_END>>\"\n",
    "                    continue # Continue the outer while loop for the next step\n",
    "\n",
    "                yield token # Yield the token to the consumer\n",
    "\n",
    "                # Accumulate tool calls to process after the step stream is done\n",
    "                if hasattr(token, \"tool_calls\") and token.tool_calls:\n",
    "                     if current_tool_call is None:\n",
    "                         current_tool_call = token\n",
    "                     else:\n",
    "                         current_tool_call += token\n",
    "\n",
    "            # Process the tool call that was accumulated AFTER the astream is done for this step\n",
    "            if current_tool_call and current_tool_call.tool_calls:\n",
    "                 tool_call_message = AIMessage(\n",
    "                     content=current_tool_call.content,\n",
    "                     tool_calls=current_tool_call.tool_calls,\n",
    "                     tool_call_id=current_tool_call.tool_calls[0][\"id\"] # Assuming one tool call per step for simplicity\n",
    "                 )\n",
    "                 agent_scratchpad.append(tool_call_message)\n",
    "\n",
    "                 # Extract tool info\n",
    "                 tool_name = tool_call_message.tool_calls[0][\"name\"]\n",
    "                 tool_args = tool_call_message.tool_calls[0][\"args\"]\n",
    "                 tool_call_id = tool_call_message.tool_calls[0][\"id\"]\n",
    "                 tool_args_final = tool_args # Keep track of the last tool args (useful for final answer)\n",
    "\n",
    "                 if verbose:\n",
    "                     print(f\"Executing tool: {tool_name} with args: {tool_args}\", flush=True)\n",
    "\n",
    "\n",
    "                 # If final_answer, we are done\n",
    "                 if tool_name == \"final_answer\":\n",
    "                     final_answer = tool_args.get(\"answer\", \"\")\n",
    "                     # The final answer is streamed as part of the last step\n",
    "                     # We don't need to execute final_answer as a Python function here\n",
    "                     # just update chat history and break\n",
    "                     self.chat_history.extend([\n",
    "                         HumanMessage(content=input),\n",
    "                         AIMessage(content=final_answer)\n",
    "                     ])\n",
    "                     # Signal done to the main streamer (though not strictly needed with generator)\n",
    "                     # streamer.queue.put_nowait(\"<<DONE>>\") # Removed, generator handles end\n",
    "                     return # End the generator\n",
    "\n",
    "                 # Execute real tool\n",
    "                 if tool_name not in name2tool:\n",
    "                     tool_out = f\"Error: Unknown tool {tool_name}\"\n",
    "                 else:\n",
    "                     try:\n",
    "                         tool_out = name2tool[tool_name](**tool_args)\n",
    "                     except Exception as e:\n",
    "                         tool_out = f\"Error executing {tool_name}: {str(e)}\"\n",
    "\n",
    "                 # Add tool result to scratchpad\n",
    "                 tool_exec = ToolMessage(\n",
    "                     content=str(tool_out),\n",
    "                     tool_call_id=tool_call_id\n",
    "                 )\n",
    "                 agent_scratchpad.append(tool_exec)\n",
    "                 count += 1\n",
    "            else:\n",
    "                # If no tool call was made in a step (shouldn't happen with tool_choice=\"any\")\n",
    "                # Treat the content as the final answer or an error\n",
    "                final_answer = current_tool_call.content if current_tool_call else \"Agent did not produce a tool call.\"\n",
    "                self.chat_history.extend([\n",
    "                    HumanMessage(content=input),\n",
    "                    AIMessage(content=final_answer)\n",
    "                ])\n",
    "                yield final_answer # Yield the final content\n",
    "                return # End the generator\n",
    "\n",
    "\n",
    "        # If loop ended without final_answer (max iterations reached)\n",
    "        if final_answer is None:\n",
    "            final_answer = \"Max iterations reached without final answer\"\n",
    "            self.chat_history.extend([\n",
    "                HumanMessage(content=input),\n",
    "                AIMessage(content=final_answer)\n",
    "            ])\n",
    "            yield final_answer # Yield the final content\n",
    "\n",
    "        # No explicit return needed for generator end\n",
    "\n",
    "agent_executor = CustomAgentExecutor(max_iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dff089",
   "metadata": {},
   "source": [
    "Although this seems like a lot of work, we're now streaming tokens in a way that allows us to pass these tokens on to other parts of our code - such as through a websocket, streamed API response, or some downstream processing.\n",
    "\n",
    "Let's try this out, we'll put together some simple post-processing to allow us to more nicely format the streamed output from out agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7090dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling exponentiate...\n",
      "{'y': 2, 'x': 5}\n",
      "Calling multiply...\n",
      "{'y': 25, 'x': 10}\n",
      "Calling add...\n",
      "{'y': 250, 'x': 10}\n",
      "Calling final_answer...\n",
      "{'tools_used': ['exponentiate', 'multiply', 'add'], 'answer': '10 + 10 multiplied by 5 with power 2 is 260.'}\n"
     ]
    }
   ],
   "source": [
    "agent_executor = CustomAgentExecutor(max_iterations=5) # Re-initialize after modification\n",
    "\n",
    "async for token in agent_executor.invoke(\"What is 10 + 10 multiplied by 5 with power 2\", QueueCallbackHandler(asyncio.Queue())):\n",
    "    if token == \"<<STEP_END>>\":\n",
    "        print(\"\\n<<STEP_END>>\\n\", flush=True)\n",
    "        continue\n",
    "\n",
    "    # Check if token has tool_calls (LangChain normalized format)\n",
    "    if hasattr(token, \"tool_calls\") and token.tool_calls:\n",
    "        tool_call = token.tool_calls[0]\n",
    "        # if we have a tool name, we'll print it\n",
    "        if tool_name := tool_call.get(\"name\"):\n",
    "            print(f\"Calling {tool_name}...\", flush=True)\n",
    "        # if we have arguments, we print them\n",
    "        if tool_args := tool_call.get(\"args\"):\n",
    "            print(f\"{tool_args}\", flush=True)\n",
    "    else:\n",
    "        # If not a tool call, it's likely the final answer content\n",
    "        print(token.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
