{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2040ca",
   "metadata": {},
   "source": [
    "# LangChain Agents Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80adfafb",
   "metadata": {},
   "source": [
    "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
    "\n",
    "In this example, we will introduce LangChain's Agents, adding the ability to use tools such as search and calculators to complete tasks that normal LLMs cannot fufil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a1aa253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env into environment\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_ENDPOINT'] = \"https://eu.api.smith.langchain.com \"\n",
    "os.environ['LANGSMITH_API_KEY'] =  os.getenv('LANGSMITH_API_KEY') or getpass('Enter your LangSmith API Key: ')\n",
    "os.environ['LANGSMITH_PROJECT'] = 'agents-intro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or getpass(\n",
    "    \"Enter GOOGLE API Key: \"\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",      \n",
    "    temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4accb459",
   "metadata": {},
   "source": [
    "## What is the Agent Executor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549a67c",
   "metadata": {},
   "source": [
    "When we talk about agents, a significant part of an \"agent\" is simple code logic,\n",
    "iteratively rerunning LLM calls and processing their output. The exact logic varies\n",
    "significantly, but one well-known example is the **ReAct** agent.\n",
    "\n",
    "![ReAct process](https://www.aurelio.ai/_next/image?url=%2Fimages%2Fposts%2Fai-agents%2Fai-agents-00.png&w=640&q=75)\n",
    "\n",
    "**Re**ason + **Act**ion (ReAct) agents use iterative _reasoning_ and _action_ steps to\n",
    "incorporate chain-of-thought and tool-use into their execution. During the _reasoning_\n",
    "step, the LLM generates the steps to take to answer the query. Next, the LLM generates\n",
    "the _action_ input, which our code logic parses into a tool call. (observation -> tool output)\n",
    "\n",
    "![Agentic graph of ReAct](https://www.aurelio.ai/_next/image?url=%2Fimages%2Fposts%2Fai-agents%2Fai-agents-01.png&w=640&q=75)\n",
    "\n",
    "Following our action step, we get an observation from the tool call. Then, we feed the\n",
    "observation back into the agent executor logic for a final answer or further reasoning\n",
    "and action steps.\n",
    "\n",
    "The agent and agent executor we will be building will follow this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160df83",
   "metadata": {},
   "source": [
    "## Introduction to Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50aa4f",
   "metadata": {},
   "source": [
    "Tools are a way to augment our LLMs with code execution. A tool is simply a function formatted so that our agent can undertstand how to use it, and then execute it. Let's start by creating a few simple tools.\n",
    "\n",
    "We can use the `@tool` decorator to create an LLM-compatible tool from a standard python function — this function should include a few things for optimal performance:\n",
    "\n",
    "* A docstring describing what the tool does and when it should be used, this will be read by our LLM/agent and used to decide when to use the tool, and also how to use the tool.\n",
    "\n",
    "* Clear parameter names that ideally tell the LLM what each parameter is, if it isn't clear we make sure the docstring explains what the parameter is for and how to use it.\n",
    "\n",
    "* Both parameter and return type annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "77409e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'y' from 'x'.\"\"\"\n",
    "    return x - y\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y    \n",
    "\n",
    "@tool\n",
    "def divide(x: float, y: float) -> float:\n",
    "    \"\"\"Divide 'x' by 'y'.\"\"\"\n",
    "    if y == 0:\n",
    "        raise ValueError(\"Cannot divide by zero.\")\n",
    "    return x / y\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded25ea",
   "metadata": {},
   "source": [
    "With the `@tool` decorator our function is turned into a `StructuredTool` object, which we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad5668c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='add', description=\"Add 'x' and 'y'.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x0000017222477240>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5d1252",
   "metadata": {},
   "source": [
    "We can see the tool name, description, and arg schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd481249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add\n",
      "Add 'x' and 'y'. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{add.name}\\n{add.description} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cfea3",
   "metadata": {},
   "source": [
    "The `args_schema` is a pydantic model that is transformed into the JSON schema and passed into our LLM, it is this that defines _how_ the tool is used for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae282cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Add 'x' and 'y'.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'add',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cd4f023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596d3ec",
   "metadata": {},
   "source": [
    "When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1ac6138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 5, 'y': 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "llm_output_string = \"{\\\"x\\\": 5, \\\"y\\\": 2}\"  # this is the output from the LLM\n",
    "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
    "llm_output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9cb5d2",
   "metadata": {},
   "source": [
    "This is then passed into the tool function as `kwargs` (keyword arguments) as indicated by the `**` operator - the `**` operator is used to unpack the dictionary into keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "759316dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponentiate.func(**llm_output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408ed7f",
   "metadata": {},
   "source": [
    "This covers the basics of tools and how they work, let's move on to creating the agent itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87ec86",
   "metadata": {},
   "source": [
    "## Creating an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051664b7",
   "metadata": {},
   "source": [
    "We will use **L**ang**C**hain **E**pression **L**anguage (LCEL) to construct the agent.\n",
    "The agent will be constructed using syntax and components like so:\n",
    "\n",
    "```\n",
    "agent = (\n",
    "    <input parameters, including chat history and user query>\n",
    "    | <prompt>\n",
    "    | <LLM with tools>\n",
    ")\n",
    "```\n",
    "\n",
    "We need this agent to remember previous interactions within the conversation. To do that, we will use the `ChatPromptTemplate` with a system message, a placeholder for our chat history, a placeholder for the user query, and finally a placeholder for the agent scratchpad.\n",
    "\n",
    "The agent scratchpad is where the agent writes its notes as it works through multiple internal thought and tool-use steps to produce a final output for the user. This scratchpad is a list of messages with alternating roles of `ai` (for the tool call) and `tool` (for the tool execution output). Both message types require a `tool_call_id` field which is used to link the respective AI and tool messages - this can be required when we many tool calls happening in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "edbde4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a helpful assistant. When answering a user's question \"\n",
    "        \"you should first use one of the tools provided. After using a \"\n",
    "        \"tool the tool output will be provided in the \"\n",
    "        \"'scratchpad' below. If you have an answer in the \"\n",
    "        \"scratchpad you should not use any more tools and \"\n",
    "        \"instead answer directly to the user.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"), # MessagesPlaceholder can used like this as well\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4260bce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You're a helpful assistant. When answering a user's question you should first use one of the tools provided. After using a tool the tool output will be provided in the 'scratchpad' below. If you have an answer in the scratchpad you should not use any more tools and instead answer directly to the user.\"), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad', optional=True)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a66f9a",
   "metadata": {},
   "source": [
    "To add tools to our LLM, we will use the `bind_tools` method within the LCEL constructor, which will take our tools and add them to the LLM. We'll also include the `tool_choice=\"any\"` argument to `bind_tools`, which tells the LLM that it _MUST_ use a tool, ie it cannot provide a final answer directly (in therefore not using a tool):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f48cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "tools = [add, subtract, multiply, divide, exponentiate]\n",
    "\n",
    "#define the agent runnable\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    } \n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4201f6c",
   "metadata": {},
   "source": [
    "We invoke the agent with the `invoke` method, passing in the input and chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "11b68e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"y\": 10, \"x\": 10}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--54649d19-ae82-4f03-9a1f-fb4bcb0034b9-0', tool_calls=[{'name': 'add', 'args': {'y': 10, 'x': 10}, 'id': 'c4bdc698-193f-46b4-b5ed-1c40b773e6c2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 325, 'output_tokens': 81, 'total_tokens': 406, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 61}})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10 + 10\", \"chat_history\": []})\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56089299",
   "metadata": {},
   "source": [
    "Because we set `tool_choice=\"any\"` to force the tool output, the usual `content` field will be empty as that field is used for natural language output, ie the _final answer_ of the LLM. To find our tool output, we need to look at the `tool_calls` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d880bb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'y': 10, 'x': 10},\n",
       "  'id': 'c4bdc698-193f-46b4-b5ed-1c40b773e6c2',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec02dbe9",
   "metadata": {},
   "source": [
    "From here, we have the tool `name` that our LLM wants to use and the `args` that it\n",
    "wants to pass to that tool. We can see that the tool `add` is being used with the\n",
    "arguments `x=10` and `y=10`. The `agent.invoke` method has _not_ executed the tool\n",
    "function; we need to write that part of the agent code ourselves.\n",
    "\n",
    "Executing the tool code requires two steps:\n",
    "\n",
    "1. Map the tool `name` to the tool function.\n",
    "\n",
    "2. Execute the tool function with the generated `args`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff8d88dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add': <function __main__.add(x: float, y: float) -> float>,\n",
       " 'subtract': <function __main__.subtract(x: float, y: float) -> float>,\n",
       " 'multiply': <function __main__.multiply(x: float, y: float) -> float>,\n",
       " 'divide': <function __main__.divide(x: float, y: float) -> float>,\n",
       " 'exponentiate': <function __main__.exponentiate(x: float, y: float) -> float>}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tool name to function mapping\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "name2tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f42b25b",
   "metadata": {},
   "source": [
    "Now execute to get our answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "acef5c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_execution_output = name2tool[tool_call.tool_calls[0][\"name\"]](**tool_call.tool_calls[0][\"args\"])\n",
    "tool_execution_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f48232",
   "metadata": {},
   "source": [
    "That is our answer and tool execution logic. We feed this back into our LLM via the\n",
    "`agent_scratchpad` placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "tool_execution = ToolMessage(\n",
    "    content = tool_execution_output,\n",
    "    tool_call_id = tool_call.tool_calls[0][\"id\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e0b97c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = agent.invoke({\n",
    "    \"input\": \"What is 10 + 10\",\n",
    "    \"chat_history\": [],\n",
    "    \"agent_scratchpad\": [tool_call, tool_execution]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9fd84dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"y\": 10, \"x\": 10}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--dc12b217-262b-495b-a11a-ddcbec16225f-0', tool_calls=[{'name': 'add', 'args': {'y': 10, 'x': 10}, 'id': '2296d988-0ea5-4e91-8d64-446c53e9bd3b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 359, 'output_tokens': 105, 'total_tokens': 464, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 85}})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596275cd",
   "metadata": {},
   "source": [
    "Despite having the answer in our `agent_scratchpad`, the LLM still tries to use the tool\n",
    "_again_. This behaviour happens because we bonded the tools to the LLM with\n",
    "`tool_choice=\"any\"`. When we set `tool_choice` to `\"any\"` or `\"required\"`, we tell the\n",
    "LLM that it _MUST_ use a tool, i.e., it cannot provide a final answer.\n",
    "\n",
    "There's two options to fix this:\n",
    "\n",
    "1. Set `tool_choice=\"auto\"` to tell the LLM that it can choose to use a tool or provide\n",
    "a final answer.\n",
    "\n",
    "2. Create a `final_answer`\n",
    "\n",
    "First, let's try option **1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d6568732",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"auto\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7c10f",
   "metadata": {},
   "source": [
    "We'll start from the start again, so `agent_scratchpad` is empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f9adeb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"y\": 10, \"x\": 10}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--2f328403-dcae-4035-9738-45c3e18cfe6e-0', tool_calls=[{'name': 'add', 'args': {'y': 10, 'x': 10}, 'id': '7c8a2221-b107-4686-b8c2-a366466e9975', 'type': 'tool_call'}], usage_metadata={'input_tokens': 429, 'output_tokens': 74, 'total_tokens': 503, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 54}})"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10 + 10\", \"chat_history\": []})\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed36fd",
   "metadata": {},
   "source": [
    "Now we execute the tool and pass it's output into the `agent_scratchpad` placeholder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a86f3e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_execution_output = name2tool[tool_call.tool_calls[0][\"name\"]](**tool_call.tool_calls[0][\"args\"])\n",
    "tool_execution_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1967e10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='20', tool_call_id='7c8a2221-b107-4686-b8c2-a366466e9975')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_execution = ToolMessage(\n",
    "    content = tool_execution_output,\n",
    "    tool_call_id = tool_call.tool_calls[0][\"id\"]\n",
    ")\n",
    "tool_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b74fa63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The answer is 20.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--a2988ddf-1e68-4ffd-b042-d71b7a171f52-0', usage_metadata={'input_tokens': 463, 'output_tokens': 7, 'total_tokens': 470, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = agent.invoke({\n",
    "    \"input\": \"What is 10 + 10\",\n",
    "    \"chat_history\": [],\n",
    "    \"agent_scratchpad\": [tool_call, tool_execution]\n",
    "})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8480ebb",
   "metadata": {},
   "source": [
    "We now have the final answer in the `content` field! This method is perfectly\n",
    "functional; however, we recommend option **2** as it provides more control over the\n",
    "agent's output.\n",
    "\n",
    "There are several reasons that option **2** can provide more control, those are:\n",
    "\n",
    "* It removes the possibility of an agent using the direct `content` field when it is not\n",
    "appropriate; for example, some LLMs (particularly smaller ones) may try to use the\n",
    "`content` field when using a tool.\n",
    "\n",
    "* We can enforce a specific structured output in our answers. Structured outputs are\n",
    "handy when we require particular fields for downstream code or multi-part answers. For\n",
    "example, a RAG agent may return a natural language answer and a list of sources used to\n",
    "generate that answer.\n",
    "\n",
    "To implement option **2**, we must create a `final_answer` tool. We will add a\n",
    "`tools_used` field to give our output some structure—in a real-world use case, we\n",
    "probably wouldn't want to generate this field, but it's useful for our example here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "93e10e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def final_answer(answer: str, tools_used: list[str]) -> str:\n",
    "    \"\"\"Use this tool to provide a final answer to the user.\n",
    "    The answer should be in natural language as this will be provided\n",
    "    to the user directly. The tools_used must include a list of tool\n",
    "    names that were used within the `scratchpad`.\"\"\"\n",
    "\n",
    "    return {\"answer\": answer, \"tools_used\": tools_used}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d334e1",
   "metadata": {},
   "source": [
    "Our `final_answer` tool _doesn't_ necessarily need to do anything; in this example,\n",
    "we're using it purely to structure our final response. We can now add this tool to our\n",
    "agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6fd4693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, subtract, multiply, divide, exponentiate, final_answer]\n",
    "\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\") # we're forcing tool use again\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d3e1c0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'y': 10, 'x': 10},\n",
       "  'id': '1690580c-c4a0-4ec3-885a-0e45ef01aa98',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10 + 10?\", \"chat_history\": []})\n",
    "tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af810c",
   "metadata": {},
   "source": [
    "We execute the tool and provide it's output to the agent again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "26cc74ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_execution_output = name2tool[tool_call.tool_calls[0][\"name\"]](**tool_call.tool_calls[0][\"args\"])\n",
    "tool_execution_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "24ab760a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'final_answer', 'arguments': '{\"tools_used\": [\"add\"], \"answer\": \"10 + 10 = 20\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--554574b1-ed89-4bd1-9b3e-b2a04944b169-0', tool_calls=[{'name': 'final_answer', 'args': {'tools_used': ['add'], 'answer': '10 + 10 = 20'}, 'id': '73fb6a1f-1f24-4d43-bbac-35954f831060', 'type': 'tool_call'}], usage_metadata={'input_tokens': 464, 'output_tokens': 81, 'total_tokens': 545, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 50}})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_execution = ToolMessage(\n",
    "    content = tool_execution_output,\n",
    "    tool_call_id = tool_call.tool_calls[0][\"id\"]\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"input\": \"What is 10 + 10?\",\n",
    "    \"chat_history\": [],\n",
    "    \"agent_scratchpad\": [tool_call, tool_execution]\n",
    "})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67eb62d",
   "metadata": {},
   "source": [
    "We see that `content` remains empty because we force tool use. But we now have the\n",
    "`final_answer` tool, which the agent executor passes via the `tool_calls` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a39b5ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'final_answer',\n",
       "  'args': {'tools_used': ['add'], 'answer': '10 + 10 = 20'},\n",
       "  'id': '73fb6a1f-1f24-4d43-bbac-35954f831060',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "607a3c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tools_used': ['add'], 'answer': '10 + 10 = 20'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls[0][\"args\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "63963579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'final_answer', 'arguments': '{\"tools_used\": [\"add\"], \"answer\": \"10 + 10 = 20\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--554574b1-ed89-4bd1-9b3e-b2a04944b169-0', tool_calls=[{'name': 'final_answer', 'args': {'tools_used': ['add'], 'answer': '10 + 10 = 20'}, 'id': '73fb6a1f-1f24-4d43-bbac-35954f831060', 'type': 'tool_call'}], usage_metadata={'input_tokens': 464, 'output_tokens': 81, 'total_tokens': 545, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 50}})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e1f0d",
   "metadata": {},
   "source": [
    "### Building a Custom Agent Execution Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b50e1a6",
   "metadata": {},
   "source": [
    "We've worked through each step of our agent code, but it doesn't run without us running\n",
    "every step. We must write a class to handle all the logic we just worked through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "962d9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: str) -> dict:\n",
    "        #invoke the agent but we do this iteratiively in a loop until reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            tool_call = self.agent.invoke({\n",
    "                \"input\": input,\n",
    "                \"chat_history\": self.chat_history,\n",
    "                \"agent_scratchpad\": agent_scratchpad\n",
    "            })\n",
    "            # add initial too call to scratchpad \n",
    "            agent_scratchpad.append(tool_call)\n",
    "\n",
    "            # we execute the tool and add the output to the scratchpad\n",
    "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "            tool_call_id = tool_call.tool_calls[0][\"id\"]\n",
    "\n",
    "            tool_execution_output = name2tool[tool_name](**tool_args)\n",
    "            tool_execution = ToolMessage(\n",
    "                content = f\"{tool_execution_output}\",\n",
    "                tool_call_id = tool_call_id\n",
    "            )\n",
    "\n",
    "            agent_scratchpad.append(tool_execution)\n",
    "\n",
    "            #add a print statement to see the intermediate steps\n",
    "            print(f\"{count}: {tool_name}({tool_args}) => {tool_execution_output}\")\n",
    "            count += 1\n",
    "\n",
    "            # check if the tool called was final_answer\n",
    "            if tool_name == \"final_answer\":\n",
    "                break\n",
    "\n",
    "        # add the final output to the chat history\n",
    "        final_output = tool_execution_output['answer']\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=final_output)\n",
    "        ])\n",
    "        \n",
    "        # return the final answer in dict form\n",
    "        return json.dumps(tool_execution_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc7e0a",
   "metadata": {},
   "source": [
    "Now initialize the agent executor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "da7c204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = CustomAgentExecutor(max_iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f658840",
   "metadata": {},
   "source": [
    "And test the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "814ebd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: add({'y': 10, 'x': 10}) => 20\n",
      "1: final_answer({'tools_used': ['add'], 'answer': '10 + 10 = 20'}) => {'answer': '10 + 10 = 20', 'tools_used': ['add']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"answer\": \"10 + 10 = 20\", \"tools_used\": [\"add\"]}'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(input=\"What is 10 + 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f84ae4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: multiply({'y': 5, 'x': 2}) => 10\n",
      "1: exponentiate({'y': 2, 'x': 10}) => 100\n",
      "2: divide({'y': 100, 'x': 100}) => 1.0\n",
      "3: add({'y': 1, 'x': 10}) => 11\n",
      "4: final_answer({'tools_used': ['multiply', 'exponentiate', 'divide', 'add'], 'answer': '10 + 100 / (2 * 5)**2 = 11'}) => {'answer': '10 + 100 / (2 * 5)**2 = 11', 'tools_used': ['multiply', 'exponentiate', 'divide', 'add']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"answer\": \"10 + 100 / (2 * 5)**2 = 11\", \"tools_used\": [\"multiply\", \"exponentiate\", \"divide\", \"add\"]}'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(input=\"What is 10 + 100 / (2 * 5)**2 \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
